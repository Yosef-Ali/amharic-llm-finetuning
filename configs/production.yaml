# Production H-Net Training Configuration
# Optimized for real Amharic corpus training

model:
  vocab_size: 32000
  hidden_dim: 512  # Increased for better capacity
  num_layers: 8    # Deeper network
  num_heads: 16    # More attention heads
  dropout: 0.1
  max_seq_len: 1024  # Longer sequences

data:
  train_path: "data/training/sentences_train.jsonl"
  val_path: "data/training/sentences_val.jsonl"
  batch_size: 16  # Optimized for memory
  max_length: 512
  num_workers: 4

train:
  epochs: 100      # Production training
  learning_rate: 2e-4
  weight_decay: 0.01
  warmup_steps: 1000
  save_steps: 500  # Save checkpoints frequently
  eval_steps: 100
  logging_steps: 50
  output_dir: "outputs/production"
  seed: 42
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0

optimizer:
  name: "adamw"
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  name: "cosine"
  num_cycles: 0.5

# Checkpointing
checkpoint:
  save_total_limit: 3
  save_strategy: "steps"
  load_best_model_at_end: true
  metric_for_best_model: "eval_loss"
  greater_is_better: false

# Logging
logging:
  report_to: ["tensorboard"]
  logging_dir: "outputs/production/logs"
  log_level: "info"

# Early stopping
early_stopping:
  patience: 5
  threshold: 0.001