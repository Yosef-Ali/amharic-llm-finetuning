#!/usr/bin/env python3
"""
Human-in-the-Loop Amharic Text Generator
Implements RLHF, contextual embeddings, and interactive refinement
"""

import torch
import torch.nn.functional as F
import numpy as np
import sys
import os
import json
import pickle
from typing import List, Dict, Tuple, Optional, Set
import re
import random
from datetime import datetime
from collections import defaultdict, Counter
import math

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from enhanced_train import EnhancedHNet, EnhancedAmharicTokenizer

class ContextualEmbedding:
    """Advanced contextual understanding for Amharic"""
    
    def __init__(self):
        # Semantic categories with contextual relationships
        self.semantic_graph = {
            '·à∞·àã·àù': {'category': 'greeting', 'context': ['morning', 'meeting', 'peace'], 
                   'follows': ['·ä•·äï·ã∞·àù·äï', '·å§·äì', '·ã∞·àÖ·äì'], 'precedes': ['·äê·àÖ', '·äê·àΩ', '·äì·âΩ·àÅ']},
            '·âµ·àù·àÖ·à≠·âµ': {'category': 'education', 'context': ['school', 'learning', 'knowledge'],
                      'follows': ['·â§·âµ', '·àò·àù·àÖ·à≠', '·â∞·àõ·à™'], 'precedes': ['·ã≠·à∞·å£·àç', '·ä†·àµ·çà·àã·åä', '·å†·âÉ·àö']},
            '·ä¢·âµ·ãÆ·åµ·ã´': {'category': 'country', 'context': ['nation', 'homeland', 'africa'],
                      'follows': ['·àÄ·åà·à≠', '·àÖ·ãù·â•', '·â£·àÖ·àç'], 'precedes': ['·ãç·â•', '·â≥·à™·ä´·ãä', '·å•·äï·â≥·ãä']},
            '·â§·â∞·à∞·â•': {'category': 'family', 'context': ['home', 'relatives', 'love'],
                     'follows': ['·ä•·äì·âµ', '·ä†·â£·âµ', '·àç·åÜ·âΩ'], 'precedes': ['·ãà·ã≥·åÖ·äê·âµ', '·çç·âÖ·à≠', '·ä†·äï·ãµ·äê·âµ']},
            '·â£·àÖ·àç': {'category': 'culture', 'context': ['tradition', 'heritage', 'customs'],
                   'follows': ['·ãà·åç', '·âã·äï·âã', '·å•·â†·â•'], 'precedes': ['·âÜ·äï·åÜ', '·å•·äï·â≥·ãä', '·ã®·â∞·àà·ã´·ã®']}
        }
        
        # Advanced sentence templates with semantic slots
        self.semantic_templates = {
            'descriptive': [
                '{subject} {adjective} {object} ·äê·ãç·ç¢',
                '{subject} ·â†·å£·àù {adjective} ·äê·ãç·ç¢',
                '·ã≠·àÖ {subject} {adjective} ·ä•·äì {adjective2} ·äê·ãç·ç¢'
            ],
            'narrative': [
                '{subject} {verb} ·ä•·äì {object} {verb2}·ç¢',
                '·â†{time} {subject} {verb} ·äê·â†·à≠·ç¢',
                '{subject} ·ä®{place} {verb} ·ä•·äì {result} ·àÜ·äê·ç¢'
            ],
            'explanatory': [
                '{subject} ·àõ·àà·âµ {definition} ·àõ·àà·âµ ·äê·ãç·ç¢',
                '{subject} {reason} ·àµ·àà·àÜ·äê {result}·ç¢',
                '·àµ·àà {subject} ·àõ·ãà·âÖ ·ã´·àà·â•·äï {fact} ·äê·ãç·ç¢'
            ]
        }
        
        # Contextual word relationships
        self.word_associations = {
            '·âµ·àù·àÖ·à≠·âµ': ['·ãï·ãç·âÄ·âµ', '·àò·àõ·à≠', '·àõ·àµ·â∞·àõ·à≠', '·å•·äì·âµ', '·àù·à≠·àù·à≠'],
            '·à∞·àã·àù': ['·ã∞·àÖ·äï·äê·âµ', '·çç·âÖ·à≠', '·ä†·äï·ãµ·äê·âµ', '·àò·åç·â£·â£·âµ', '·àò·â∞·à≥·à∞·â•'],
            '·ä¢·âµ·ãÆ·åµ·ã´': ['·ä†·çç·à™·ä´', '·â≥·à™·ä≠', '·â£·àÖ·àç', '·âã·äï·âã', '·àÖ·ãù·â•'],
            '·â§·â∞·à∞·â•': ['·çç·âÖ·à≠', '·ä•·äï·ä≠·â•·ä´·â§', '·àò·â∞·à≥·à∞·â•', '·ä†·â•·àÆ·äê·âµ', '·àò·ä®·â£·â†·à≠'],
            '·â£·àÖ·àç': ['·ãà·åç', '·àç·àõ·ãµ', '·å•·â†·â•', '·ã≥·äï·àµ', '·àô·ãö·âÉ']
        }

class HumanFeedbackCollector:
    """Collects and processes human feedback for RLHF"""
    
    def __init__(self, feedback_file: str = "human_feedback.json"):
        self.feedback_file = feedback_file
        self.feedback_data = self.load_feedback()
        
    def load_feedback(self) -> Dict:
        """Load existing feedback data"""
        try:
            with open(self.feedback_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            return {'ratings': [], 'preferences': [], 'corrections': []}
    
    def save_feedback(self):
        """Save feedback data"""
        with open(self.feedback_file, 'w', encoding='utf-8') as f:
            json.dump(self.feedback_data, f, ensure_ascii=False, indent=2)
    
    def collect_rating(self, prompt: str, generated_text: str, rating: int, comments: str = ""):
        """Collect human rating for generated text"""
        feedback_entry = {
            'timestamp': datetime.now().isoformat(),
            'prompt': prompt,
            'generated_text': generated_text,
            'rating': rating,  # 1-5 scale
            'comments': comments
        }
        self.feedback_data['ratings'].append(feedback_entry)
        self.save_feedback()
    
    def collect_preference(self, prompt: str, text_a: str, text_b: str, preference: str, reason: str = ""):
        """Collect preference between two texts"""
        preference_entry = {
            'timestamp': datetime.now().isoformat(),
            'prompt': prompt,
            'text_a': text_a,
            'text_b': text_b,
            'preference': preference,  # 'a', 'b', or 'tie'
            'reason': reason
        }
        self.feedback_data['preferences'].append(preference_entry)
        self.save_feedback()
    
    def collect_correction(self, prompt: str, generated_text: str, corrected_text: str, correction_type: str):
        """Collect human corrections"""
        correction_entry = {
            'timestamp': datetime.now().isoformat(),
            'prompt': prompt,
            'generated_text': generated_text,
            'corrected_text': corrected_text,
            'correction_type': correction_type  # 'grammar', 'meaning', 'style', 'factual'
        }
        self.feedback_data['corrections'].append(correction_entry)
        self.save_feedback()
    
    def get_feedback_insights(self) -> Dict:
        """Analyze collected feedback for insights"""
        insights = {
            'avg_rating': 0,
            'common_issues': [],
            'preferred_patterns': [],
            'improvement_areas': []
        }
        
        if self.feedback_data['ratings']:
            ratings = [entry['rating'] for entry in self.feedback_data['ratings']]
            insights['avg_rating'] = np.mean(ratings)
        
        # Analyze common correction types
        correction_types = [entry['correction_type'] for entry in self.feedback_data['corrections']]
        insights['common_issues'] = list(Counter(correction_types).most_common(3))
        
        return insights

class AdvancedAmharicGenerator:
    """Advanced generator with human-in-the-loop capabilities"""
    
    def __init__(self, model_path: str = "models/enhanced_hnet/best_model.pt", 
                 tokenizer_path: str = "models/enhanced_tokenizer.pkl"):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.load_model(model_path, tokenizer_path)
        
        self.contextual_embedding = ContextualEmbedding()
        self.feedback_collector = HumanFeedbackCollector()
        
        # Advanced generation parameters
        self.generation_strategies = {
            'creative': {'temperature': 0.9, 'top_p': 0.8, 'repetition_penalty': 1.3},
            'balanced': {'temperature': 0.7, 'top_p': 0.9, 'repetition_penalty': 1.2},
            'conservative': {'temperature': 0.5, 'top_p': 0.95, 'repetition_penalty': 1.1}
        }
        
        # Semantic coherence weights learned from feedback
        self.coherence_weights = self.load_coherence_weights()
        
    def load_model(self, model_path: str, tokenizer_path: str):
        """Load model and tokenizer"""
        try:
            self.tokenizer = EnhancedAmharicTokenizer()
            self.tokenizer.load(tokenizer_path)
            
            self.model = EnhancedHNet(
                vocab_size=self.tokenizer.vocab_size,
                embedding_dim=256,
                hidden_dim=512,
                num_layers=3,
                dropout=0.2
            )
            
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            
            print("‚úÖ Advanced generator loaded successfully!")
            
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            raise
    
    def load_coherence_weights(self) -> Dict:
        """Load learned coherence weights from feedback"""
        try:
            with open('coherence_weights.json', 'r') as f:
                return json.load(f)
        except FileNotFoundError:
            return {
                'semantic_consistency': 0.3,
                'grammatical_correctness': 0.25,
                'contextual_relevance': 0.25,
                'creativity': 0.2
            }
    
    def save_coherence_weights(self):
        """Save updated coherence weights"""
        with open('coherence_weights.json', 'w') as f:
            json.dump(self.coherence_weights, f, indent=2)
    
    def generate_with_context(self, prompt: str, context_type: str = 'auto', 
                            strategy: str = 'balanced', max_length: int = 100) -> List[Dict]:
        """Generate text with contextual understanding"""
        
        # Determine context if auto
        if context_type == 'auto':
            context_type = self.detect_context(prompt)
        
        # Generate multiple candidates
        candidates = []
        
        for i in range(3):
            # Use different strategies for diversity
            current_strategy = list(self.generation_strategies.keys())[i % 3]
            
            if context_type == 'semantic':
                text = self.generate_semantic_text(prompt, current_strategy, max_length)
            elif context_type == 'narrative':
                text = self.generate_narrative_text(prompt, current_strategy, max_length)
            else:
                text = self.generate_contextual_text(prompt, current_strategy, max_length)
            
            # Score the candidate
            score = self.score_text_quality(prompt, text, context_type)
            
            candidates.append({
                'text': text,
                'strategy': current_strategy,
                'context_type': context_type,
                'quality_score': score,
                'metadata': self.analyze_text_features(text)
            })
        
        # Sort by quality score
        candidates.sort(key=lambda x: x['quality_score'], reverse=True)
        return candidates
    
    def detect_context(self, prompt: str) -> str:
        """Detect the most appropriate context for the prompt"""
        semantic_graph = self.contextual_embedding.semantic_graph
        
        if prompt in semantic_graph:
            category = semantic_graph[prompt]['category']
            if category in ['greeting', 'family']:
                return 'narrative'
            elif category in ['education', 'culture']:
                return 'semantic'
            else:
                return 'descriptive'
        
        return 'semantic'  # Default
    
    def generate_semantic_text(self, prompt: str, strategy: str, max_length: int) -> str:
        """Generate semantically coherent text"""
        semantic_graph = self.contextual_embedding.semantic_graph
        word_associations = self.contextual_embedding.word_associations
        
        if prompt in semantic_graph:
            # Use semantic relationships
            related_words = semantic_graph[prompt].get('follows', [])
            associations = word_associations.get(prompt, [])
            
            # Build semantic sentence
            if related_words and associations:
                # Choose template based on context
                templates = self.contextual_embedding.semantic_templates['explanatory']
                template = random.choice(templates)
                
                # Fill template with semantic content
                if '{subject}' in template:
                    template = template.replace('{subject}', prompt)
                if '{definition}' in template:
                    template = template.replace('{definition}', random.choice(associations))
                if '{fact}' in template:
                    template = template.replace('{fact}', random.choice(related_words))
                
                return template
        
        # Fallback to model generation with semantic guidance
        return self.generate_with_model(prompt, strategy, max_length, semantic_boost=True)
    
    def generate_narrative_text(self, prompt: str, strategy: str, max_length: int) -> str:
        """Generate narrative-style text"""
        templates = self.contextual_embedding.semantic_templates['narrative']
        template = random.choice(templates)
        
        # Fill narrative template
        semantic_graph = self.contextual_embedding.semantic_graph
        
        if prompt in semantic_graph:
            follows = semantic_graph[prompt].get('follows', ['·äê·åà·à≠'])
            precedes = semantic_graph[prompt].get('precedes', ['·äê·ãç'])
            
            template = template.replace('{subject}', prompt)
            if '{verb}' in template:
                template = template.replace('{verb}', random.choice(['·ã≠·à∞·à´·àç', '·ã≠·àÜ·äì·àç', '·ã≠·åà·äõ·àç']))
            if '{object}' in template:
                template = template.replace('{object}', random.choice(follows))
            if '{result}' in template:
                template = template.replace('{result}', random.choice(precedes))
            
            return template
        
        return self.generate_with_model(prompt, strategy, max_length)
    
    def generate_contextual_text(self, prompt: str, strategy: str, max_length: int) -> str:
        """Generate contextually appropriate text"""
        templates = self.contextual_embedding.semantic_templates['descriptive']
        template = random.choice(templates)
        
        # Use contextual information
        semantic_graph = self.contextual_embedding.semantic_graph
        
        if prompt in semantic_graph:
            context = semantic_graph[prompt].get('context', [])
            
            template = template.replace('{subject}', prompt)
            if '{adjective}' in template:
                adjectives = ['·å•·à©', '·ãç·â•', '·ä†·àµ·çà·àã·åä', '·å†·âÉ·àö', '·àç·ã©']
                template = template.replace('{adjective}', random.choice(adjectives))
            if '{object}' in template:
                template = template.replace('{object}', random.choice(context) if context else '·äê·åà·à≠')
            
            return template
        
        return self.generate_with_model(prompt, strategy, max_length)
    
    def generate_with_model(self, prompt: str, strategy: str, max_length: int, semantic_boost: bool = False) -> str:
        """Generate using the neural model with advanced sampling"""
        params = self.generation_strategies[strategy]
        
        input_ids = self.tokenizer.encode(prompt)
        if not input_ids:
            input_ids = [0]
        
        input_tensor = torch.tensor([input_ids], device=self.device)
        generated_ids = input_ids.copy()
        hidden = None
        
        with torch.no_grad():
            for step in range(max_length - len(prompt)):
                logits, hidden = self.model(input_tensor, hidden)
                logits = logits[:, -1, :]
                
                # Apply semantic boosting if requested
                if semantic_boost:
                    logits = self.apply_semantic_boost(logits, prompt, generated_ids)
                
                # Apply advanced sampling
                next_token_id = self.advanced_sampling(
                    logits, 
                    temperature=params['temperature'],
                    top_p=params['top_p'],
                    repetition_penalty=params['repetition_penalty'],
                    generated_ids=generated_ids
                )
                
                next_char = self.tokenizer.decode([next_token_id])
                generated_ids.append(next_token_id)
                
                input_tensor = torch.cat([
                    input_tensor, 
                    torch.tensor([[next_token_id]], device=self.device)
                ], dim=1)
                
                # Stop at sentence end
                if next_char in ['·ç¢', '!', '?'] and len(generated_ids) > len(input_ids) + 10:
                    break
        
        return self.tokenizer.decode(generated_ids)
    
    def apply_semantic_boost(self, logits: torch.Tensor, prompt: str, generated_ids: List[int]) -> torch.Tensor:
        """Boost semantically relevant tokens"""
        word_associations = self.contextual_embedding.word_associations
        
        if prompt in word_associations:
            for word in word_associations[prompt]:
                word_ids = self.tokenizer.encode(word)
                for token_id in word_ids:
                    if token_id < logits.size(-1):
                        logits[0, token_id] *= 1.5  # Boost semantic words
        
        return logits
    
    def advanced_sampling(self, logits: torch.Tensor, temperature: float, top_p: float, 
                         repetition_penalty: float, generated_ids: List[int]) -> int:
        """Advanced sampling with multiple techniques"""
        # Apply repetition penalty
        for token_id in set(generated_ids[-20:]):  # Last 20 tokens
            if token_id < logits.size(-1):
                logits[0, token_id] /= repetition_penalty
        
        # Temperature scaling
        logits = logits / temperature
        
        # Nucleus (top-p) sampling
        sorted_logits, sorted_indices = torch.sort(logits, descending=True)
        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)
        
        # Remove tokens with cumulative probability above the threshold
        sorted_indices_to_remove = cumulative_probs > top_p
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
        logits[indices_to_remove] = float('-inf')
        
        # Sample
        probs = F.softmax(logits, dim=-1)
        next_token_id = torch.multinomial(probs, num_samples=1).item()
        
        return next_token_id
    
    def score_text_quality(self, prompt: str, text: str, context_type: str) -> float:
        """Score text quality using multiple metrics"""
        scores = {
            'semantic_consistency': self.score_semantic_consistency(prompt, text),
            'grammatical_correctness': self.score_grammar(text),
            'contextual_relevance': self.score_contextual_relevance(prompt, text, context_type),
            'creativity': self.score_creativity(text)
        }
        
        # Weighted average using learned weights
        total_score = sum(scores[metric] * self.coherence_weights[metric] 
                         for metric in scores)
        
        return total_score
    
    def score_semantic_consistency(self, prompt: str, text: str) -> float:
        """Score semantic consistency"""
        word_associations = self.contextual_embedding.word_associations
        
        if prompt not in word_associations:
            return 0.5  # Neutral score
        
        associated_words = word_associations[prompt]
        words_in_text = text.split()
        
        # Count semantic matches
        matches = sum(1 for word in words_in_text 
                     if any(assoc in word for assoc in associated_words))
        
        return min(matches / len(words_in_text), 1.0) if words_in_text else 0
    
    def score_grammar(self, text: str) -> float:
        """Score grammatical correctness (simplified)"""
        # Check for proper sentence structure
        sentences = text.split('·ç¢')
        if not sentences:
            return 0
        
        # Basic checks
        has_proper_ending = text.endswith(('·ç¢', '!', '?'))
        avg_word_length = np.mean([len(word) for word in text.split()]) if text.split() else 0
        
        # Reasonable word length (2-8 characters for Amharic)
        word_length_score = 1.0 if 2 <= avg_word_length <= 8 else 0.5
        
        return (0.6 * word_length_score + 0.4 * (1.0 if has_proper_ending else 0.5))
    
    def score_contextual_relevance(self, prompt: str, text: str, context_type: str) -> float:
        """Score contextual relevance"""
        semantic_graph = self.contextual_embedding.semantic_graph
        
        if prompt not in semantic_graph:
            return 0.5
        
        expected_context = semantic_graph[prompt]['context']
        follows = semantic_graph[prompt].get('follows', [])
        
        # Check if text contains contextually relevant words
        relevance_score = 0
        words_in_text = text.lower().split()
        
        for word in words_in_text:
            if any(ctx in word for ctx in expected_context):
                relevance_score += 0.3
            if word in follows:
                relevance_score += 0.5
        
        return min(relevance_score, 1.0)
    
    def score_creativity(self, text: str) -> float:
        """Score creativity and diversity"""
        words = text.split()
        if len(words) < 2:
            return 0
        
        # Lexical diversity
        unique_words = len(set(words))
        diversity_score = unique_words / len(words)
        
        # Character diversity
        unique_chars = len(set(text))
        char_diversity = unique_chars / len(text) if text else 0
        
        return (diversity_score + char_diversity) / 2
    
    def analyze_text_features(self, text: str) -> Dict:
        """Analyze various text features"""
        words = text.split()
        
        return {
            'word_count': len(words),
            'char_count': len(text),
            'avg_word_length': np.mean([len(word) for word in words]) if words else 0,
            'unique_words': len(set(words)),
            'lexical_diversity': len(set(words)) / len(words) if words else 0,
            'has_punctuation': any(p in text for p in ['·ç¢', '·ç£', '·ç§', '!', '?']),
            'sentence_count': len([s for s in text.split('·ç¢') if s.strip()])
        }
    
    def interactive_refinement(self, prompt: str, initial_candidates: List[Dict]) -> Dict:
        """Interactive refinement with human feedback"""
        print(f"\nüéØ Interactive Refinement for: '{prompt}'")
        print("=" * 50)
        
        for i, candidate in enumerate(initial_candidates):
            print(f"\n{i+1}. Strategy: {candidate['strategy']} | Score: {candidate['quality_score']:.3f}")
            print(f"   Text: '{candidate['text']}'")
            print(f"   Features: {candidate['metadata']['word_count']} words, "
                  f"{candidate['metadata']['lexical_diversity']:.2f} diversity")
        
        # Simulate human feedback (in real implementation, this would be interactive)
        print("\nüìù Collecting feedback...")
        
        # For demo, simulate preference for highest scoring candidate
        best_candidate = initial_candidates[0]
        
        # Simulate rating
        simulated_rating = min(5, max(1, int(best_candidate['quality_score'] * 5)))
        
        self.feedback_collector.collect_rating(
            prompt=prompt,
            generated_text=best_candidate['text'],
            rating=simulated_rating,
            comments=f"Generated with {best_candidate['strategy']} strategy"
        )
        
        return best_candidate
    
    def update_from_feedback(self):
        """Update model parameters based on collected feedback"""
        insights = self.feedback_collector.get_feedback_insights()
        
        # Adjust coherence weights based on feedback
        if insights['avg_rating'] < 3.0:
            # Increase focus on grammatical correctness
            self.coherence_weights['grammatical_correctness'] += 0.05
            self.coherence_weights['creativity'] -= 0.05
        elif insights['avg_rating'] > 4.0:
            # Increase creativity if quality is high
            self.coherence_weights['creativity'] += 0.03
            self.coherence_weights['semantic_consistency'] -= 0.03
        
        # Normalize weights
        total_weight = sum(self.coherence_weights.values())
        for key in self.coherence_weights:
            self.coherence_weights[key] /= total_weight
        
        self.save_coherence_weights()
        
        print(f"üìä Updated weights based on {len(self.feedback_collector.feedback_data['ratings'])} ratings")
        print(f"   Average rating: {insights['avg_rating']:.2f}")
        print(f"   New weights: {self.coherence_weights}")

def demo_human_in_loop():
    """Demonstrate human-in-the-loop generation"""
    print("ü§ñ Human-in-the-Loop Amharic Text Generation")
    print("Advanced contextual understanding with RLHF")
    print("=" * 60)
    
    try:
        generator = AdvancedAmharicGenerator()
        
        test_prompts = [
            "·â£·àÖ·àç",
            "·à∞·àã·àù", 
            "·âµ·àù·àÖ·à≠·âµ",
            "·ä¢·âµ·ãÆ·åµ·ã´"
        ]
        
        for prompt in test_prompts:
            print(f"\nüìù Prompt: '{prompt}'")
            print("-" * 40)
            
            # Generate candidates with context
            candidates = generator.generate_with_context(
                prompt=prompt,
                context_type='auto',
                strategy='balanced',
                max_length=80
            )
            
            # Interactive refinement
            best_result = generator.interactive_refinement(prompt, candidates)
            
            print(f"\n‚úÖ Final Result:")
            print(f"   '{best_result['text']}'")
            print(f"   Quality Score: {best_result['quality_score']:.3f}")
            print(f"   Context: {best_result['context_type']}")
        
        # Update from feedback
        print("\n" + "="*60)
        generator.update_from_feedback()
        
        print("\nüéâ Human-in-the-loop demo complete!")
        print("‚úÖ Features demonstrated:")
        print("   ‚Ä¢ Contextual text generation")
        print("   ‚Ä¢ Multiple generation strategies")
        print("   ‚Ä¢ Quality scoring and ranking")
        print("   ‚Ä¢ Human feedback collection")
        print("   ‚Ä¢ Adaptive weight adjustment")
        
    except Exception as e:
        print(f"‚ùå Demo failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    demo_human_in_loop()