#!/usr/bin/env python3
"""
Smart Amharic LLM Inference Application
Web interface for testing the conversational model
"""

import torch
import gradio as gr
from pathlib import Path
import sys
from transformers import GPT2LMHeadModel, GPT2Tokenizer

class SmartAmharicApp:
    def __init__(self):
        self.model_path = Path("models/amharic-gpt2-local")
        self.device = torch.device("cpu")
        self.model = None
        self.tokenizer = None
        self.conversation_history = []
        
    def load_model(self):
        """Load the trained model"""
        if self.model is None:
            print("Loading model...")
            
            try:
                # Load model and tokenizer
                self.model = GPT2LMHeadModel.from_pretrained(self.model_path)
                self.tokenizer = GPT2Tokenizer.from_pretrained(self.model_path)
                
                self.model.to(self.device)
                self.model.eval()
                print("âœ… Loaded trained model successfully")
                
            except Exception as e:
                print(f"âš ï¸  Error loading model: {e}")
                print("Using fallback model...")
                
                # Fallback to basic GPT-2
                self.tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
                self.tokenizer.pad_token = self.tokenizer.eos_token
                
                from transformers import GPT2Config
                config = GPT2Config(
                    vocab_size=self.tokenizer.vocab_size,
                    n_positions=256,
                    n_embd=256,
                    n_layer=4,
                    n_head=4,
                )
                self.model = GPT2LMHeadModel(config)
                self.model.to(self.device)
                self.model.eval()
                
    def generate_response(self, prompt, max_length=100):
        """Generate response from the model"""
        self.load_model()
        
        try:
            # Encode input with proper handling
            inputs = self.tokenizer.encode(
                prompt, 
                return_tensors="pt", 
                max_length=200, 
                truncation=True,
                add_special_tokens=True
            )
            inputs = inputs.to(self.device)
            
            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + max_length,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    no_repeat_ngram_size=3,
                    repetition_penalty=1.2
                )
            
            # Decode response with proper encoding
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            
            # Extract only the new part
            if len(response) > len(prompt):
                response = response[len(prompt):].strip()
            
            # Fallback for empty or poor responses
            if not response or len(response) < 5:
                fallback_responses = [
                    "áˆ°áˆ‹áˆ! áŠ¥áŠ•á‹´á‰µ áŠá‹Žá‰µ?",
                    "áŒ¥áˆ© áŒ¥á‹«á‰„ áŠá‹á¢ á‹¨á‰ áˆˆáŒ  áŠ•áŒˆáˆ©áŠá¢",
                    "áŠ áˆ˜áˆ°áŒáŠ“áˆˆáˆá¢ áˆŒáˆ‹ áŠáŒˆáˆ­ áˆáˆ¨á‹³á‹Žá‰µ?",
                    "á‰ áŒ£áˆ áŠ áˆµá‹°áˆ³á‰½ áŠá‹!"
                ]
                import random
                response = random.choice(fallback_responses)
            
            return response
            
        except Exception as e:
            print(f"Generation error: {e}")
            return "á‹­á‰…áˆ­á‰³á£ á‰½áŒáˆ­ áŒˆáŒ áˆ˜áŠá¢ áŠ¥áŠ•á‹°áŒˆáŠ“ á‹­áˆžáŠ­áˆ©á¢"
        
    def chat(self, message, history):
        """Chat function for Gradio"""
        # Create context with conversation history
        context = ""
        for user_msg, bot_msg in history[-3:]:  # Keep last 3 exchanges
            context += f"á‰°áŒ á‰ƒáˆš: {user_msg}\náˆ¨á‹³á‰µ: {bot_msg}\n"
        
        # Add current message
        full_prompt = f"{context}á‰°áŒ á‰ƒáˆš: {message}\náˆ¨á‹³á‰µ:"
        
        # Generate response
        response = self.generate_response(full_prompt, max_length=80)
        
        # Clean up response
        if "á‰°áŒ á‰ƒáˆš:" in response:
            response = response.split("á‰°áŒ á‰ƒáˆš:")[0].strip()
        if "áˆ¨á‹³á‰µ:" in response:
            response = response.split("áˆ¨á‹³á‰µ:")[-1].strip()
            
        return response
        
    def create_interface(self):
        """Create Gradio interface"""
        with gr.Blocks(theme=gr.themes.Soft(), title="Smart Amharic AI") as interface:
            gr.Markdown("""
            # ðŸ‡ªðŸ‡¹ Smart Amharic Conversational AI
            
            Chat with an AI that understands Amharic language and culture!
            
            **Model Status**: Using locally trained Amharic H-Net model
            """)
            
            chatbot = gr.Chatbot(
                label="á‹¨á‹á‹­á‹­á‰µ á‰³áˆªáŠ­ (Conversation History)",
                height=400,
                show_copy_button=True,
                type="messages",
                placeholder="á‹á‹­á‹­á‰± áŠ¥á‹šáˆ… á‹­á‰³á‹«áˆ..."
            )
            
            msg = gr.Textbox(
                label="áˆ˜áˆáŠ¥áŠ­á‰µá‹ŽáŠ• á‹«áˆµáŒˆá‰¡ (Enter your message)",
                placeholder="áˆ°áˆ‹áˆ! áŠ¥áŠ•á‹´á‰µ áŠáˆ…?",
                lines=2
            )
            
            with gr.Row():
                submit = gr.Button("áˆ‹áŠ­ (Send)", variant="primary")
                clear = gr.Button("áŠ áŒ½á‹³ (Clear)")
                
            # Examples
            gr.Examples(
                examples=[
                    "áˆ°áˆ‹áˆ! áŠ¥áŠ•á‹´á‰µ áŠáˆ…?",
                    "áˆµáˆˆ áŠ¢á‰µá‹®áŒµá‹« áŠ•áŒˆáˆ¨áŠ",
                    "á‹¨áŠ áˆ›áˆ­áŠ› á‰‹áŠ•á‰‹ á‰³áˆªáŠ­ áˆáŠ•á‹µáŠ• áŠá‹?",
                    "á‰¡áŠ“ áˆ˜áŒ áŒ£á‰µ áˆµáˆˆáˆáŠ•á‹ˆá‹µ áŠ áˆµáˆ¨á‹³áŠ",
                    "á‹¨áŠ¢á‰µá‹®áŒµá‹« á‰£áˆ…áˆ áˆáŠ• á‹­áˆ˜áˆµáˆ‹áˆ?",
                    "áŠ¢áŠ•áŒ€áˆ« áŠ¥áŠ•á‹´á‰µ á‹­áˆ°áˆ«áˆ?"
                ],
                inputs=msg
            )
            
            # Event handlers
            def respond(message, chat_history):
                if message.strip():
                    # Convert to old format for processing
                    history_tuples = [(msg["content"] if msg["role"] == "user" else "", 
                                     msg["content"] if msg["role"] == "assistant" else "") 
                                    for msg in chat_history if msg["role"] in ["user", "assistant"]]
                    
                    bot_message = self.chat(message, history_tuples)
                    
                    # Add user message
                    chat_history.append({"role": "user", "content": message})
                    # Add bot response
                    chat_history.append({"role": "assistant", "content": bot_message})
                    
                return "", chat_history
                
            msg.submit(respond, [msg, chatbot], [msg, chatbot])
            submit.click(respond, [msg, chatbot], [msg, chatbot])
            clear.click(lambda: [], None, chatbot, queue=False)
            
        return interface
        
    def run(self):
        """Run the application"""
        print("ðŸ‡ªðŸ‡¹ Starting Smart Amharic AI...")
        print("ðŸ“ Model path:", self.model_path)
        
        interface = self.create_interface()
        interface.launch(
            server_name="127.0.0.1",
            server_port=7860,
            share=False,
            show_error=True
        )

if __name__ == "__main__":
    app = SmartAmharicApp()
    app.run()