#!/usr/bin/env python3
"""Evaluate Amharic H-Net model performance."""

import sys
from pathlib import Path
import json
import random
from typing import Dict

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from amharichnet.evaluation import AmharicTextEvaluator, evaluate_generation_quality
from generate import AmharicGenerator


class ModelEvaluator:
    """Comprehensive model evaluation."""
    
    def __init__(self):
        self.evaluator = AmharicTextEvaluator()
        self.generator = AmharicGenerator()
        
        # Test prompts for evaluation
        self.test_prompts = [
            "",  # Empty prompt
            "·ä¢·âµ·ãÆ·åµ·ã´",
            "·ä†·ã≤·àµ ·ä†·â†·â£", 
            "·à∞·àã·àù ·ãà·äï·ãµ·àú",
            "·âµ·àù·àÖ·à≠·âµ",
            "·ãà·å£·â∂·âΩ",
            "·â£·àÖ·àç",
            "·àÄ·åà·à≠",
            "·àÖ·ãù·â•",
            "·àò·äï·åç·à•·âµ"
        ]
        
        # Reference texts (high-quality Amharic)
        self.reference_texts = [
            "·ä¢·âµ·ãÆ·åµ·ã´ ·â†·ä†·çç·à™·ä´ ·âÄ·äï·ãµ ·ã®·àù·âµ·åà·äù ·ãç·â• ·àÄ·åà·à≠ ·äì·âµ·ç¢ ·â•·ãô ·â•·àî·àÆ·âΩ ·ä•·äì ·â•·àî·à®·à∞·â¶·âΩ ·â†·à∞·àã·àù ·ã®·àö·äñ·à©·â£·âµ ·àÄ·åà·à≠ ·äì·âµ·ç¢",
            "·ä†·ã≤·àµ ·ä†·â†·â£ ·ã®·ä¢·âµ·ãÆ·åµ·ã´ ·ãã·äì ·ä®·â∞·àõ ·ä•·äì ·ã®·ä†·çç·à™·ä´ ·ã≤·çï·àé·àõ·à≤·ã´·ãä ·ãã·äì ·ä®·â∞·àõ ·äì·âµ·ç¢ ·â•·ãô ·ãì·àà·àù ·ä†·âÄ·çç ·ãµ·à≠·åÖ·â∂·âΩ ·ãã·äì ·àò·à•·à™·ã´ ·â§·âµ ·äì·âµ·ç¢",
            "·âµ·àù·àÖ·à≠·âµ ·àà·àÅ·àâ·àù ·àÖ·åª·äì·âµ ·àò·â•·âµ ·äê·ãç·ç¢ ·â†·å•·à´·âµ ·âµ·àù·àÖ·à≠·âµ ·àÄ·åà·à™·â±·äï ·àà·àò·åà·äï·â£·âµ ·ãà·à≥·äù ·äê·ãç·ç¢",
            "·à∞·àã·àù ·àà·àÅ·àâ·àù ·à∞·ãç ·ä†·àµ·çà·àã·åä ·äê·ãç·ç¢ ·â†·à∞·àã·àù ·ä•·äì ·â†·àò·ä®·â£·â†·à≠ ·ã®·àù·äï·äñ·à≠·â†·âµ ·àõ·äÖ·â†·à®·à∞·â• ·àò·åà·äï·â£·âµ ·ä†·àà·â•·äï·ç¢",
            "·ãà·å£·â∂·âΩ ·ã®·àÄ·åà·à™·â± ·â∞·àµ·çã ·ä•·äì ·ã®·ãà·ã∞·çä·âµ ·àò·à™·ãé·âΩ ·äì·â∏·ãç·ç¢ ·â†·âµ·àù·àÖ·à≠·âµ ·ä•·äì ·â†·à•·à´ ·àã·ã≠ ·àõ·â∞·äÆ·à≠ ·ä†·àà·â£·â∏·ãç·ç¢"
        ]
    
    def evaluate_generation_quality(self, num_samples: int = 50) -> Dict:
        """Evaluate generation quality with multiple samples."""
        print(f"üîç Generating {num_samples} samples for evaluation...")
        
        generated_texts = []
        
        # Generate texts with different prompts
        for i in range(num_samples):
            # Mix of empty prompts and specific prompts
            if i % 3 == 0:
                prompt = ""  # Empty prompt
                category = random.choice(["general", "news", "educational", "cultural"])
            else:
                prompt = random.choice(self.test_prompts[1:])  # Skip empty
                category = "general"
            
            text = self.generator.generate_text(
                category=category,
                prompt=prompt,
                length=random.randint(30, 80)
            )
            generated_texts.append(text)
        
        print(f"‚úÖ Generated {len(generated_texts)} samples")
        
        # Evaluate against references
        results = evaluate_generation_quality(generated_texts, self.reference_texts)
        
        return results
    
    def evaluate_specific_prompts(self) -> Dict:
        """Evaluate generation for specific prompts."""
        print(f"üéØ Evaluating specific prompts...")
        
        prompt_results = {}
        
        for prompt in self.test_prompts:
            print(f"   Testing prompt: '{prompt}'")
            
            # Generate multiple samples for this prompt
            samples = []
            for _ in range(5):
                text = self.generator.generate_text(prompt=prompt, length=50)
                samples.append(text)
            
            # Evaluate samples
            evaluation = self.evaluator.evaluate_multiple(samples)
            
            prompt_results[prompt if prompt else "empty_prompt"] = {
                'samples': samples,
                'evaluation': evaluation,
                'best_sample': samples[evaluation['best_text_index']],
                'avg_quality': evaluation['statistics']['overall_quality']['mean']
            }
        
        return prompt_results
    
    def compare_categories(self) -> Dict:
        """Compare generation quality across categories."""
        print(f"üìä Comparing generation across categories...")
        
        categories = ["general", "news", "educational", "cultural", "conversation"]
        category_results = {}
        
        for category in categories:
            print(f"   Testing category: {category}")
            
            samples = []
            for _ in range(10):
                prompt = random.choice(self.test_prompts[1:5])  # Use some prompts
                text = self.generator.generate_text(
                    category=category,
                    prompt=prompt,
                    length=50
                )
                samples.append(text)
            
            evaluation = self.evaluator.evaluate_multiple(samples)
            category_results[category] = evaluation
        
        return category_results
    
    def test_model_consistency(self, prompt: str = "·ä¢·âµ·ãÆ·åµ·ã´", num_runs: int = 20) -> Dict:
        """Test model consistency with same prompt."""
        print(f"üîÑ Testing consistency with prompt '{prompt}' ({num_runs} runs)...")
        
        results = []
        for i in range(num_runs):
            text = self.generator.generate_text(prompt=prompt, length=40)
            score = self.evaluator.evaluate_text(text)
            results.append({
                'run': i + 1,
                'text': text,
                'scores': score
            })
        
        # Calculate consistency metrics
        qualities = [r['scores']['overall_quality'] for r in results]
        lengths = [r['scores']['length'] for r in results]
        
        import statistics
        consistency_metrics = {
            'quality_mean': statistics.mean(qualities),
            'quality_std': statistics.stdev(qualities) if len(qualities) > 1 else 0,
            'length_mean': statistics.mean(lengths),
            'length_std': statistics.stdev(lengths) if len(lengths) > 1 else 0,
            'unique_outputs': len(set(r['text'] for r in results)),
            'total_runs': num_runs
        }
        
        return {
            'results': results,
            'consistency': consistency_metrics
        }
    
    def run_comprehensive_evaluation(self) -> Dict:
        """Run comprehensive model evaluation."""
        print("üöÄ Starting Comprehensive Model Evaluation")
        print("=" * 50)
        
        evaluation_results = {}
        
        # 1. Overall generation quality
        print("\n1Ô∏è‚É£ Overall Generation Quality")
        evaluation_results['overall_quality'] = self.evaluate_generation_quality(30)
        self._print_quality_summary(evaluation_results['overall_quality'])
        
        # 2. Prompt-specific evaluation
        print("\n2Ô∏è‚É£ Prompt-Specific Evaluation")
        evaluation_results['prompt_evaluation'] = self.evaluate_specific_prompts()
        self._print_prompt_summary(evaluation_results['prompt_evaluation'])
        
        # 3. Category comparison
        print("\n3Ô∏è‚É£ Category Comparison")
        evaluation_results['category_comparison'] = self.compare_categories()
        self._print_category_summary(evaluation_results['category_comparison'])
        
        # 4. Consistency testing
        print("\n4Ô∏è‚É£ Model Consistency")
        evaluation_results['consistency'] = self.test_model_consistency()
        self._print_consistency_summary(evaluation_results['consistency'])
        
        # 5. Save results
        output_file = "evaluation_results.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            # Convert for JSON serialization
            serializable_results = self._make_serializable(evaluation_results)
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nüíæ Results saved to {output_file}")
        
        return evaluation_results
    
    def _print_quality_summary(self, results):
        """Print summary of quality evaluation."""
        stats = results['generated']['statistics']
        print(f"   üìà Generated Text Quality:")
        print(f"      Overall Quality: {stats['overall_quality']['mean']:.3f} ¬± {stats['overall_quality']['std']:.3f}")
        print(f"      Amharic Ratio: {stats['amharic_ratio']['mean']:.3f}")
        print(f"      Fluency Score: {stats['fluency_score']['mean']:.3f}")
        print(f"      Coherence Score: {stats['coherence_score']['mean']:.3f}")
        
        if 'reference' in results:
            print(f"   üìä vs Reference Texts:")
            comp = results['comparison']
            for metric in ['overall_quality', 'fluency_score', 'coherence_score']:
                improvement = comp[metric]['improvement']
                print(f"      {metric}: {improvement:+.1%}")
    
    def _print_prompt_summary(self, results):
        """Print summary of prompt evaluation."""
        print(f"   üéØ Best performing prompts:")
        sorted_prompts = sorted(results.items(), key=lambda x: x[1]['avg_quality'], reverse=True)
        for prompt, data in sorted_prompts[:3]:
            print(f"      '{prompt}': {data['avg_quality']:.3f}")
            print(f"         Best: '{data['best_sample'][:50]}...'")
    
    def _print_category_summary(self, results):
        """Print summary of category comparison."""
        print(f"   üìÇ Category performance:")
        for category, data in results.items():
            avg_quality = data['statistics']['overall_quality']['mean']
            print(f"      {category}: {avg_quality:.3f}")
    
    def _print_consistency_summary(self, results):
        """Print summary of consistency test."""
        consistency = results['consistency']
        print(f"   üîÑ Consistency metrics:")
        print(f"      Quality std: {consistency['quality_std']:.3f}")
        print(f"      Unique outputs: {consistency['unique_outputs']}/{consistency['total_runs']}")
        print(f"      Diversity: {consistency['unique_outputs']/consistency['total_runs']:.1%}")
    
    def _make_serializable(self, obj):
        """Make object JSON serializable."""
        if isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif hasattr(obj, '__dict__'):
            return self._make_serializable(obj.__dict__)
        else:
            return obj


def main():
    print("üìä Amharic H-Net Model Evaluation")
    print("=" * 40)
    
    evaluator = ModelEvaluator()
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "--quick":
            # Quick evaluation
            results = evaluator.evaluate_generation_quality(10)
            print(f"\nüìà Quick Evaluation Results:")
            stats = results['generated']['statistics']
            print(f"   Overall Quality: {stats['overall_quality']['mean']:.3f}")
            print(f"   Fluency: {stats['fluency_score']['mean']:.3f}")
            print(f"   Coherence: {stats['coherence_score']['mean']:.3f}")
        elif sys.argv[1] == "--consistency":
            # Consistency test only
            results = evaluator.test_model_consistency()
            print(f"\nüîÑ Consistency Results:")
            print(f"   Quality std: {results['consistency']['quality_std']:.3f}")
            print(f"   Unique outputs: {results['consistency']['unique_outputs']}/{results['consistency']['total_runs']}")
    else:
        # Full evaluation
        results = evaluator.run_comprehensive_evaluation()
        
        print(f"\n‚úÖ Comprehensive evaluation complete!")
        print(f"   Results saved to evaluation_results.json")


if __name__ == "__main__":
    main()