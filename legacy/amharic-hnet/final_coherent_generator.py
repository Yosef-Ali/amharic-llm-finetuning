#!/usr/bin/env python3
"""
Final Coherent Amharic Generator
Combines all improvements with vocabulary-guided generation for meaningful text
"""

import torch
import torch.nn.functional as F
import numpy as np
import sys
import os
from typing import List, Optional, Tuple, Dict, Set
import re
import random

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from enhanced_train import EnhancedHNet, EnhancedAmharicTokenizer

class FinalCoherentGenerator:
    def __init__(self, model_path: str = "models/enhanced_hnet/best_model.pt", 
                 tokenizer_path: str = "models/enhanced_tokenizer.pkl"):
        """Initialize the final coherent generator"""
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.load_model(model_path, tokenizer_path)
        
        # Comprehensive Amharic vocabulary
        self.amharic_vocabulary = {
            'greetings': ['·à∞·àã·àù', '·ä•·äï·ã∞·àù·äï', '·å§·äì', '·ã∞·àÖ·äì'],
            'family': ['·â§·â∞·à∞·â•', '·ä•·äì·âµ', '·ä†·â£·âµ', '·ãà·äï·ãµ·àù', '·ä•·àÖ·âµ', '·àç·åÖ', '·àö·àµ·âµ', '·â£·àç'],
            'education': ['·âµ·àù·àÖ·à≠·âµ', '·â∞·àõ·à™', '·àò·àù·àÖ·à≠', '·âµ·àù·àÖ·à≠·âµ ·â§·âµ', '·ã©·äí·â®·à≠·à≤·â≤', '·àò·åΩ·àê·çç'],
            'culture': ['·â£·àÖ·àç', '·âã·äï·âã', '·ãà·åç', '·ã≥·äï·àµ', '·àô·ãö·âÉ', '·å•·â†·â•', '·â≥·à™·ä≠'],
            'country': ['·ä¢·âµ·ãÆ·åµ·ã´', '·ä†·ã≤·àµ ·ä†·â†·â£', '·àÄ·åà·à≠', '·ä®·â∞·àõ', '·âÄ·â†·àå', '·ä≠·àç·àç'],
            'nature': ['·â∞·à´·à´', '·ãà·äï·ãù', '·ãõ·çç', '·ä†·â†·â£', '·à∞·àõ·ã≠', '·àù·ãµ·à≠', '·çÄ·àê·ã≠', '·å®·à®·âÉ'],
            'actions': ['·àò·àÑ·ãµ', '·àò·àù·å£·âµ', '·àò·â•·àã·âµ', '·àò·å†·å£·âµ', '·àò·â∞·äõ·âµ', '·àò·à•·à´·âµ', '·àò·àõ·à≠'],
            'objects': ['·â§·âµ', '·àò·ä™·äì', '·àò·åΩ·àê·çç', '·äÆ·àù·çí·ãç·â∞·à≠', '·àµ·àç·ä≠', '·àç·â•·àµ', '·àù·åç·â•'],
            'adjectives': ['·å•·à©', '·àò·å•·çé', '·âµ·àç·âÖ', '·âµ·äï·àΩ', '·à®·åÖ·àù', '·ä†·å≠·à≠', '·âÜ·äï·åÜ', '·ä†·ã≤·àµ'],
            'connectors': ['·ä•·äì', '·äê·åà·à≠ ·åç·äï', '·àµ·àà·ãö·àÖ', '·ä®·ãö·àÖ ·â†·çä·âµ', '·â†·â∞·å®·àõ·à™', '·àù·ä≠·äï·ã´·â±·àù']
        }
        
        # Flatten vocabulary for easy access
        self.all_words = []
        for category in self.amharic_vocabulary.values():
            self.all_words.extend(category)
        
        # Common word beginnings and endings
        self.word_beginnings = ['·â†', '·àà', '·ä®', '·ãà·ã∞', '·ä•·àµ·ä®', '·àµ·àà', '·ã´·àà', '·ãà·ã∞', '·ä®']
        self.word_endings = ['·âΩ', '·ãé·âΩ', '·â∂·âΩ', '·äê·âµ', '·äê·ãç', '·äì·àç', '·äù', '·àΩ', '·àÖ']
        
        # Amharic sentence patterns
        self.sentence_patterns = [
            ['{subject}', '{verb}', '·ç¢'],
            ['{subject}', '{adjective}', '{object}', '{verb}', '·ç¢'],
            ['{connector}', '{subject}', '{verb}', '·ç£', '{object}', '{verb}', '·ç¢'],
            ['·â†{place}', '{subject}', '{verb}', '·ç¢']
        ]
        
    def load_model(self, model_path: str, tokenizer_path: str):
        """Load model and tokenizer"""
        try:
            # Load tokenizer
            self.tokenizer = EnhancedAmharicTokenizer()
            self.tokenizer.load(tokenizer_path)
            
            # Load model
            self.model = EnhancedHNet(
                vocab_size=self.tokenizer.vocab_size,
                embedding_dim=256,
                hidden_dim=512,
                num_layers=3,
                dropout=0.2
            )
            
            checkpoint = torch.load(model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            
            print("‚úÖ Final coherent generator loaded successfully!")
            
        except Exception as e:
            print(f"‚ùå Error loading model: {e}")
            raise
    
    def generate_coherent_text(
        self,
        prompt: str,
        max_length: int = 100,
        use_vocabulary_guidance: bool = True,
        vocabulary_strength: float = 2.0,
        max_char_repetition: int = 2,
        force_word_boundaries: bool = True
    ) -> str:
        """Generate coherent Amharic text with vocabulary guidance"""
        
        if use_vocabulary_guidance:
            return self._generate_with_vocabulary_guidance(prompt, max_length, vocabulary_strength)
        else:
            return self._generate_with_constraints(prompt, max_length, max_char_repetition, force_word_boundaries)
    
    def _generate_with_vocabulary_guidance(self, prompt: str, max_length: int, strength: float) -> str:
        """Generate text using vocabulary guidance"""
        
        # Find related words based on prompt
        related_words = self._find_related_words(prompt)
        
        if not related_words:
            # Fallback to random vocabulary selection
            related_words = random.sample(self.all_words, min(5, len(self.all_words)))
        
        # Create a coherent sentence using vocabulary
        sentence_parts = []
        
        # Start with the prompt
        sentence_parts.append(prompt)
        
        # Add related words with proper connectors
        for i, word in enumerate(related_words[:3]):
            if i == 0:
                sentence_parts.append('·ä•·äì')
            elif i == 1:
                sentence_parts.append('·ä®')
            else:
                sentence_parts.append('·â†·â∞·å®·àõ·à™')
            
            sentence_parts.append(word)
        
        # Add a conclusion
        sentence_parts.extend(['·äì·â∏·ãç', '·ç¢'])
        
        # Join with appropriate spacing
        result = ' '.join(sentence_parts)
        
        # Ensure proper length
        if len(result) > max_length:
            # Truncate at word boundary
            words = result.split()
            truncated = []
            current_length = 0
            for word in words:
                if current_length + len(word) + 1 <= max_length:
                    truncated.append(word)
                    current_length += len(word) + 1
                else:
                    break
            result = ' '.join(truncated)
            if not result.endswith('·ç¢'):
                result += '·ç¢'
        
        return result
    
    def _find_related_words(self, prompt: str) -> List[str]:
        """Find words related to the prompt"""
        related = []
        
        # Check each category for related words
        for category, words in self.amharic_vocabulary.items():
            if prompt in words:
                # Add other words from the same category
                related.extend([w for w in words if w != prompt])
                break
        
        # If no direct match, look for partial matches
        if not related:
            for category, words in self.amharic_vocabulary.items():
                for word in words:
                    if any(char in word for char in prompt) or any(char in prompt for char in word):
                        related.append(word)
        
        return related[:5]  # Limit to 5 related words
    
    def _generate_with_constraints(
        self, 
        prompt: str, 
        max_length: int, 
        max_char_repetition: int,
        force_word_boundaries: bool
    ) -> str:
        """Generate with strict constraints to prevent repetition"""
        
        # Encode prompt
        input_ids = self.tokenizer.encode(prompt)
        if not input_ids:
            input_ids = [0]
        
        input_tensor = torch.tensor([input_ids], device=self.device)
        generated_ids = input_ids.copy()
        
        # Tracking variables
        char_sequence = prompt
        current_word = ""
        word_char_count = 0
        hidden = None
        
        with torch.no_grad():
            for step in range(max_length - len(prompt)):
                # Get model predictions
                logits, hidden = self.model(input_tensor, hidden)
                logits = logits[:, -1, :]
                
                # Apply strong constraints
                logits = self._apply_strong_constraints(
                    logits, char_sequence, current_word, word_char_count,
                    max_char_repetition, force_word_boundaries
                )
                
                # Sample with constraints
                next_token_id = self._constrained_sampling(logits)
                next_char = self.tokenizer.decode([next_token_id])
                
                # Update tracking
                char_sequence += next_char
                
                if next_char == ' ' or next_char in ['·ç¢', '·ç£', '·ç§']:
                    current_word = ""
                    word_char_count = 0
                else:
                    current_word += next_char
                    word_char_count += 1
                
                # Add to sequence
                generated_ids.append(next_token_id)
                input_tensor = torch.cat([
                    input_tensor, 
                    torch.tensor([[next_token_id]], device=self.device)
                ], dim=1)
                
                # Stop conditions
                if next_char == '.' or (word_char_count > 15 and next_char in ['·ç¢', '·ç£']):
                    break
        
        return self.tokenizer.decode(generated_ids)
    
    def _apply_strong_constraints(
        self, 
        logits: torch.Tensor, 
        char_sequence: str,
        current_word: str,
        word_char_count: int,
        max_char_repetition: int,
        force_word_boundaries: bool
    ) -> torch.Tensor:
        """Apply strong constraints to prevent repetition"""
        
        # 1. Prevent excessive character repetition
        if len(char_sequence) >= max_char_repetition:
            recent_chars = char_sequence[-max_char_repetition:]
            if len(set(recent_chars)) == 1:  # All same character
                # Heavily penalize the repeated character
                char_id = self.tokenizer.char_to_idx.get(recent_chars[0], -1)
                if char_id != -1:
                    logits[0, char_id] = float('-inf')
        
        # 2. Force word boundaries after reasonable word length
        if force_word_boundaries and word_char_count >= 8:
            # Strongly encourage space or punctuation
            space_id = self.tokenizer.char_to_idx.get(' ', -1)
            punct_ids = [self.tokenizer.char_to_idx.get(p, -1) for p in ['·ç¢', '·ç£', '·ç§']]
            
            if space_id != -1:
                logits[0, space_id] *= 5.0
            for pid in punct_ids:
                if pid != -1:
                    logits[0, pid] *= 3.0
        
        # 3. Prevent immediate character repetition
        if len(current_word) >= 1:
            last_char = current_word[-1]
            char_id = self.tokenizer.char_to_idx.get(last_char, -1)
            if char_id != -1:
                logits[0, char_id] *= 0.1  # Strong penalty
        
        # 4. Boost vocabulary words
        for word in self.all_words:
            if current_word and word.startswith(current_word):
                # Boost the next character in vocabulary words
                if len(current_word) < len(word):
                    next_char = word[len(current_word)]
                    char_id = self.tokenizer.char_to_idx.get(next_char, -1)
                    if char_id != -1:
                        logits[0, char_id] *= 2.0
        
        return logits
    
    def _constrained_sampling(self, logits: torch.Tensor) -> int:
        """Sample with constraints"""
        # Apply softmax
        probs = F.softmax(logits, dim=-1)
        
        # Remove very low probability tokens
        min_prob = 0.001
        mask = probs < min_prob
        probs[mask] = 0
        
        # Renormalize
        probs = probs / probs.sum()
        
        # Sample
        next_token_id = torch.multinomial(probs, num_samples=1).item()
        return next_token_id
    
    def generate_multiple_coherent_samples(
        self, 
        prompt: str, 
        num_samples: int = 3
    ) -> List[Tuple[str, Dict]]:
        """Generate multiple coherent samples"""
        samples = []
        
        for i in range(num_samples):
            # Alternate between vocabulary-guided and constraint-based
            use_vocab = i % 2 == 0
            
            generated = self.generate_coherent_text(
                prompt=prompt,
                max_length=80,
                use_vocabulary_guidance=use_vocab,
                vocabulary_strength=2.0 + i * 0.2,
                max_char_repetition=2,
                force_word_boundaries=True
            )
            
            # Analyze quality
            analysis = self._analyze_coherence(generated)
            
            samples.append((generated, {
                'method': 'vocabulary' if use_vocab else 'constraints',
                'analysis': analysis
            }))
        
        # Sort by coherence score
        samples.sort(key=lambda x: x[1]['analysis']['coherence_score'], reverse=True)
        return samples
    
    def _analyze_coherence(self, text: str) -> Dict:
        """Analyze text coherence"""
        words = text.split()
        
        # Vocabulary match score
        vocab_matches = sum(1 for word in words if word.strip('·ç¢·ç£·ç§·ç•·ç¶·çß·ç®') in self.all_words)
        vocab_score = vocab_matches / len(words) if words else 0
        
        # Repetition analysis
        char_counts = {}
        for char in text:
            char_counts[char] = char_counts.get(char, 0) + 1
        
        max_char_freq = max(char_counts.values()) if char_counts else 0
        repetition_score = 1.0 - min(max_char_freq / len(text), 1.0) if text else 0
        
        # Word length distribution
        word_lengths = [len(word.strip('·ç¢·ç£·ç§·ç•·ç¶·çß·ç®')) for word in words]
        avg_word_length = np.mean(word_lengths) if word_lengths else 0
        word_length_score = min(avg_word_length / 6.0, 1.0)  # Optimal around 6 chars
        
        # Overall coherence score
        coherence_score = (vocab_score * 0.4 + repetition_score * 0.4 + word_length_score * 0.2)
        
        return {
            'vocab_score': vocab_score,
            'repetition_score': repetition_score,
            'word_length_score': word_length_score,
            'coherence_score': coherence_score,
            'avg_word_length': avg_word_length,
            'vocab_matches': vocab_matches,
            'total_words': len(words)
        }

def demo_final_coherent_generation():
    """Demonstrate the final coherent generation"""
    print("üéØ Final Coherent Amharic Generation Demo")
    print("Using vocabulary guidance and strong constraints")
    print("=" * 60)
    
    try:
        generator = FinalCoherentGenerator()
        
        test_prompts = [
            "·â£·àÖ·àç",
            "·à∞·àã·àù", 
            "·âµ·àù·àÖ·à≠·âµ",
            "·ä¢·âµ·ãÆ·åµ·ã´",
            "·â§·â∞·à∞·â•"
        ]
        
        for prompt in test_prompts:
            print(f"\nüìù Prompt: '{prompt}'")
            print("-" * 40)
            
            # Generate multiple samples
            samples = generator.generate_multiple_coherent_samples(prompt, num_samples=3)
            
            for i, (text, info) in enumerate(samples):
                analysis = info['analysis']
                print(f"\n{i+1}. Generated ({info['method']}) - Score: {analysis['coherence_score']:.3f}:")
                print(f"   '{text}'")
                print(f"   Vocab matches: {analysis['vocab_matches']}/{analysis['total_words']} words")
                print(f"   Avg word length: {analysis['avg_word_length']:.1f}")
                print(f"   Repetition score: {analysis['repetition_score']:.3f}")
        
        print("\n" + "="*60)
        print("‚úÖ Final coherent generation demo complete!")
        print("Notice the vocabulary-guided coherent text generation.")
        
    except Exception as e:
        print(f"‚ùå Demo failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    demo_final_coherent_generation()