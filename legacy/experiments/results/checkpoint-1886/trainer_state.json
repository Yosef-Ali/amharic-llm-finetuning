{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 1886,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005302226935312832,
      "grad_norm": 13.319778442382812,
      "learning_rate": 4.5e-06,
      "loss": 10.6126,
      "step": 10
    },
    {
      "epoch": 0.010604453870625663,
      "grad_norm": 9.855400085449219,
      "learning_rate": 9.5e-06,
      "loss": 10.2208,
      "step": 20
    },
    {
      "epoch": 0.015906680805938492,
      "grad_norm": 6.583913326263428,
      "learning_rate": 1.45e-05,
      "loss": 9.6634,
      "step": 30
    },
    {
      "epoch": 0.021208907741251327,
      "grad_norm": 5.171725749969482,
      "learning_rate": 1.9500000000000003e-05,
      "loss": 9.2504,
      "step": 40
    },
    {
      "epoch": 0.026511134676564158,
      "grad_norm": 5.14556360244751,
      "learning_rate": 2.45e-05,
      "loss": 8.9678,
      "step": 50
    },
    {
      "epoch": 0.031813361611876985,
      "grad_norm": 5.413451194763184,
      "learning_rate": 2.95e-05,
      "loss": 8.6984,
      "step": 60
    },
    {
      "epoch": 0.03711558854718982,
      "grad_norm": 5.470088958740234,
      "learning_rate": 3.45e-05,
      "loss": 8.3564,
      "step": 70
    },
    {
      "epoch": 0.042417815482502653,
      "grad_norm": 5.167384147644043,
      "learning_rate": 3.9500000000000005e-05,
      "loss": 8.0491,
      "step": 80
    },
    {
      "epoch": 0.04772004241781548,
      "grad_norm": 4.978545665740967,
      "learning_rate": 4.4500000000000004e-05,
      "loss": 7.7802,
      "step": 90
    },
    {
      "epoch": 0.053022269353128315,
      "grad_norm": 6.038403034210205,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 7.5223,
      "step": 100
    },
    {
      "epoch": 0.05832449628844114,
      "grad_norm": 5.1834235191345215,
      "learning_rate": 4.974804031354983e-05,
      "loss": 7.2738,
      "step": 110
    },
    {
      "epoch": 0.06362672322375397,
      "grad_norm": 5.0284743309021,
      "learning_rate": 4.946808510638298e-05,
      "loss": 7.0503,
      "step": 120
    },
    {
      "epoch": 0.0689289501590668,
      "grad_norm": 5.250970840454102,
      "learning_rate": 4.918812989921613e-05,
      "loss": 6.7723,
      "step": 130
    },
    {
      "epoch": 0.07423117709437964,
      "grad_norm": 5.423679351806641,
      "learning_rate": 4.890817469204927e-05,
      "loss": 6.5953,
      "step": 140
    },
    {
      "epoch": 0.07953340402969247,
      "grad_norm": 5.160279750823975,
      "learning_rate": 4.862821948488242e-05,
      "loss": 6.363,
      "step": 150
    },
    {
      "epoch": 0.08483563096500531,
      "grad_norm": 5.09263277053833,
      "learning_rate": 4.834826427771557e-05,
      "loss": 6.1013,
      "step": 160
    },
    {
      "epoch": 0.09013785790031813,
      "grad_norm": 5.013623237609863,
      "learning_rate": 4.806830907054872e-05,
      "loss": 5.8474,
      "step": 170
    },
    {
      "epoch": 0.09544008483563096,
      "grad_norm": 5.036355495452881,
      "learning_rate": 4.7788353863381865e-05,
      "loss": 5.6352,
      "step": 180
    },
    {
      "epoch": 0.1007423117709438,
      "grad_norm": 5.220094203948975,
      "learning_rate": 4.750839865621501e-05,
      "loss": 5.4008,
      "step": 190
    },
    {
      "epoch": 0.10604453870625663,
      "grad_norm": 5.484769821166992,
      "learning_rate": 4.7228443449048155e-05,
      "loss": 5.132,
      "step": 200
    },
    {
      "epoch": 0.11134676564156946,
      "grad_norm": 4.908422470092773,
      "learning_rate": 4.69484882418813e-05,
      "loss": 4.8853,
      "step": 210
    },
    {
      "epoch": 0.11664899257688228,
      "grad_norm": 4.930267333984375,
      "learning_rate": 4.6668533034714445e-05,
      "loss": 4.6845,
      "step": 220
    },
    {
      "epoch": 0.12195121951219512,
      "grad_norm": 4.530440807342529,
      "learning_rate": 4.63885778275476e-05,
      "loss": 4.4487,
      "step": 230
    },
    {
      "epoch": 0.12725344644750794,
      "grad_norm": 4.693507671356201,
      "learning_rate": 4.610862262038074e-05,
      "loss": 4.2376,
      "step": 240
    },
    {
      "epoch": 0.1325556733828208,
      "grad_norm": 4.3516082763671875,
      "learning_rate": 4.582866741321389e-05,
      "loss": 3.9701,
      "step": 250
    },
    {
      "epoch": 0.1378579003181336,
      "grad_norm": 4.231184005737305,
      "learning_rate": 4.554871220604703e-05,
      "loss": 3.7832,
      "step": 260
    },
    {
      "epoch": 0.14316012725344646,
      "grad_norm": 4.191493034362793,
      "learning_rate": 4.526875699888018e-05,
      "loss": 3.5904,
      "step": 270
    },
    {
      "epoch": 0.14846235418875928,
      "grad_norm": 3.9849822521209717,
      "learning_rate": 4.498880179171333e-05,
      "loss": 3.3735,
      "step": 280
    },
    {
      "epoch": 0.1537645811240721,
      "grad_norm": 3.7406585216522217,
      "learning_rate": 4.470884658454648e-05,
      "loss": 3.2101,
      "step": 290
    },
    {
      "epoch": 0.15906680805938495,
      "grad_norm": 3.4208385944366455,
      "learning_rate": 4.4428891377379625e-05,
      "loss": 3.0568,
      "step": 300
    },
    {
      "epoch": 0.16436903499469777,
      "grad_norm": 3.213024616241455,
      "learning_rate": 4.414893617021277e-05,
      "loss": 2.8934,
      "step": 310
    },
    {
      "epoch": 0.16967126193001061,
      "grad_norm": 3.1316211223602295,
      "learning_rate": 4.3868980963045915e-05,
      "loss": 2.7638,
      "step": 320
    },
    {
      "epoch": 0.17497348886532343,
      "grad_norm": 2.9738059043884277,
      "learning_rate": 4.358902575587906e-05,
      "loss": 2.6452,
      "step": 330
    },
    {
      "epoch": 0.18027571580063625,
      "grad_norm": 3.0729615688323975,
      "learning_rate": 4.330907054871221e-05,
      "loss": 2.5006,
      "step": 340
    },
    {
      "epoch": 0.1855779427359491,
      "grad_norm": 2.3609414100646973,
      "learning_rate": 4.3029115341545354e-05,
      "loss": 2.4459,
      "step": 350
    },
    {
      "epoch": 0.19088016967126192,
      "grad_norm": 2.7977795600891113,
      "learning_rate": 4.27491601343785e-05,
      "loss": 2.3809,
      "step": 360
    },
    {
      "epoch": 0.19618239660657477,
      "grad_norm": 2.896514654159546,
      "learning_rate": 4.246920492721165e-05,
      "loss": 2.2923,
      "step": 370
    },
    {
      "epoch": 0.2014846235418876,
      "grad_norm": 2.399712562561035,
      "learning_rate": 4.218924972004479e-05,
      "loss": 2.2409,
      "step": 380
    },
    {
      "epoch": 0.2067868504772004,
      "grad_norm": 2.904630422592163,
      "learning_rate": 4.190929451287794e-05,
      "loss": 2.1535,
      "step": 390
    },
    {
      "epoch": 0.21208907741251326,
      "grad_norm": 2.809436082839966,
      "learning_rate": 4.162933930571109e-05,
      "loss": 2.1363,
      "step": 400
    },
    {
      "epoch": 0.21739130434782608,
      "grad_norm": 2.1793553829193115,
      "learning_rate": 4.134938409854424e-05,
      "loss": 2.0727,
      "step": 410
    },
    {
      "epoch": 0.22269353128313893,
      "grad_norm": 2.116676092147827,
      "learning_rate": 4.106942889137738e-05,
      "loss": 2.0528,
      "step": 420
    },
    {
      "epoch": 0.22799575821845175,
      "grad_norm": 2.2339465618133545,
      "learning_rate": 4.078947368421053e-05,
      "loss": 2.0042,
      "step": 430
    },
    {
      "epoch": 0.23329798515376457,
      "grad_norm": 2.037550449371338,
      "learning_rate": 4.0509518477043675e-05,
      "loss": 1.9746,
      "step": 440
    },
    {
      "epoch": 0.23860021208907742,
      "grad_norm": 2.147484302520752,
      "learning_rate": 4.0229563269876824e-05,
      "loss": 1.9462,
      "step": 450
    },
    {
      "epoch": 0.24390243902439024,
      "grad_norm": 1.9657416343688965,
      "learning_rate": 3.994960806270997e-05,
      "loss": 1.927,
      "step": 460
    },
    {
      "epoch": 0.2492046659597031,
      "grad_norm": 1.7472968101501465,
      "learning_rate": 3.9669652855543114e-05,
      "loss": 1.888,
      "step": 470
    },
    {
      "epoch": 0.2545068928950159,
      "grad_norm": 2.0328805446624756,
      "learning_rate": 3.938969764837626e-05,
      "loss": 1.8446,
      "step": 480
    },
    {
      "epoch": 0.2598091198303287,
      "grad_norm": 1.6890486478805542,
      "learning_rate": 3.910974244120941e-05,
      "loss": 1.8347,
      "step": 490
    },
    {
      "epoch": 0.2651113467656416,
      "grad_norm": 2.537245035171509,
      "learning_rate": 3.882978723404255e-05,
      "loss": 1.8399,
      "step": 500
    },
    {
      "epoch": 0.2704135737009544,
      "grad_norm": 1.568960189819336,
      "learning_rate": 3.854983202687571e-05,
      "loss": 1.8055,
      "step": 510
    },
    {
      "epoch": 0.2757158006362672,
      "grad_norm": 1.5317199230194092,
      "learning_rate": 3.826987681970885e-05,
      "loss": 1.7893,
      "step": 520
    },
    {
      "epoch": 0.28101802757158006,
      "grad_norm": 2.7190663814544678,
      "learning_rate": 3.7989921612542e-05,
      "loss": 1.7695,
      "step": 530
    },
    {
      "epoch": 0.2863202545068929,
      "grad_norm": 1.7535492181777954,
      "learning_rate": 3.770996640537514e-05,
      "loss": 1.7488,
      "step": 540
    },
    {
      "epoch": 0.2916224814422057,
      "grad_norm": 2.016211748123169,
      "learning_rate": 3.743001119820829e-05,
      "loss": 1.736,
      "step": 550
    },
    {
      "epoch": 0.29692470837751855,
      "grad_norm": 1.6968706846237183,
      "learning_rate": 3.7150055991041436e-05,
      "loss": 1.7537,
      "step": 560
    },
    {
      "epoch": 0.3022269353128314,
      "grad_norm": 1.8860563039779663,
      "learning_rate": 3.6870100783874584e-05,
      "loss": 1.7452,
      "step": 570
    },
    {
      "epoch": 0.3075291622481442,
      "grad_norm": 2.942594051361084,
      "learning_rate": 3.659014557670773e-05,
      "loss": 1.7241,
      "step": 580
    },
    {
      "epoch": 0.31283138918345704,
      "grad_norm": 2.0953290462493896,
      "learning_rate": 3.6310190369540874e-05,
      "loss": 1.7218,
      "step": 590
    },
    {
      "epoch": 0.3181336161187699,
      "grad_norm": 2.1111652851104736,
      "learning_rate": 3.603023516237402e-05,
      "loss": 1.6847,
      "step": 600
    },
    {
      "epoch": 0.32343584305408274,
      "grad_norm": 2.1839680671691895,
      "learning_rate": 3.5750279955207164e-05,
      "loss": 1.6824,
      "step": 610
    },
    {
      "epoch": 0.32873806998939553,
      "grad_norm": 2.6818647384643555,
      "learning_rate": 3.547032474804032e-05,
      "loss": 1.6783,
      "step": 620
    },
    {
      "epoch": 0.3340402969247084,
      "grad_norm": 2.9177935123443604,
      "learning_rate": 3.519036954087346e-05,
      "loss": 1.6814,
      "step": 630
    },
    {
      "epoch": 0.33934252386002123,
      "grad_norm": 2.5094258785247803,
      "learning_rate": 3.491041433370661e-05,
      "loss": 1.6807,
      "step": 640
    },
    {
      "epoch": 0.344644750795334,
      "grad_norm": 2.3512790203094482,
      "learning_rate": 3.463045912653976e-05,
      "loss": 1.669,
      "step": 650
    },
    {
      "epoch": 0.34994697773064687,
      "grad_norm": 2.4392127990722656,
      "learning_rate": 3.43505039193729e-05,
      "loss": 1.6602,
      "step": 660
    },
    {
      "epoch": 0.3552492046659597,
      "grad_norm": 2.055685043334961,
      "learning_rate": 3.407054871220605e-05,
      "loss": 1.6517,
      "step": 670
    },
    {
      "epoch": 0.3605514316012725,
      "grad_norm": 1.6831958293914795,
      "learning_rate": 3.3790593505039196e-05,
      "loss": 1.6289,
      "step": 680
    },
    {
      "epoch": 0.36585365853658536,
      "grad_norm": 1.9810376167297363,
      "learning_rate": 3.3510638297872344e-05,
      "loss": 1.6205,
      "step": 690
    },
    {
      "epoch": 0.3711558854718982,
      "grad_norm": 2.684884548187256,
      "learning_rate": 3.3230683090705486e-05,
      "loss": 1.6399,
      "step": 700
    },
    {
      "epoch": 0.37645811240721105,
      "grad_norm": 1.4801514148712158,
      "learning_rate": 3.2950727883538634e-05,
      "loss": 1.6093,
      "step": 710
    },
    {
      "epoch": 0.38176033934252385,
      "grad_norm": 1.888796329498291,
      "learning_rate": 3.267077267637178e-05,
      "loss": 1.5935,
      "step": 720
    },
    {
      "epoch": 0.3870625662778367,
      "grad_norm": 2.688408613204956,
      "learning_rate": 3.2390817469204924e-05,
      "loss": 1.6013,
      "step": 730
    },
    {
      "epoch": 0.39236479321314954,
      "grad_norm": 2.533860683441162,
      "learning_rate": 3.211086226203808e-05,
      "loss": 1.6218,
      "step": 740
    },
    {
      "epoch": 0.39766702014846234,
      "grad_norm": 1.9824941158294678,
      "learning_rate": 3.183090705487122e-05,
      "loss": 1.5471,
      "step": 750
    },
    {
      "epoch": 0.4029692470837752,
      "grad_norm": 2.1510207653045654,
      "learning_rate": 3.155095184770437e-05,
      "loss": 1.5917,
      "step": 760
    },
    {
      "epoch": 0.40827147401908803,
      "grad_norm": 2.415968894958496,
      "learning_rate": 3.127099664053752e-05,
      "loss": 1.5718,
      "step": 770
    },
    {
      "epoch": 0.4135737009544008,
      "grad_norm": 2.030656337738037,
      "learning_rate": 3.099104143337066e-05,
      "loss": 1.5822,
      "step": 780
    },
    {
      "epoch": 0.4188759278897137,
      "grad_norm": 2.0137369632720947,
      "learning_rate": 3.0711086226203814e-05,
      "loss": 1.5828,
      "step": 790
    },
    {
      "epoch": 0.4241781548250265,
      "grad_norm": 2.059046745300293,
      "learning_rate": 3.0431131019036956e-05,
      "loss": 1.5681,
      "step": 800
    },
    {
      "epoch": 0.42948038176033937,
      "grad_norm": 3.1681747436523438,
      "learning_rate": 3.0151175811870104e-05,
      "loss": 1.5629,
      "step": 810
    },
    {
      "epoch": 0.43478260869565216,
      "grad_norm": 3.011805295944214,
      "learning_rate": 2.987122060470325e-05,
      "loss": 1.5495,
      "step": 820
    },
    {
      "epoch": 0.440084835630965,
      "grad_norm": 2.112584114074707,
      "learning_rate": 2.9591265397536394e-05,
      "loss": 1.5658,
      "step": 830
    },
    {
      "epoch": 0.44538706256627786,
      "grad_norm": 2.7958126068115234,
      "learning_rate": 2.931131019036954e-05,
      "loss": 1.5341,
      "step": 840
    },
    {
      "epoch": 0.45068928950159065,
      "grad_norm": 2.328176975250244,
      "learning_rate": 2.903135498320269e-05,
      "loss": 1.5331,
      "step": 850
    },
    {
      "epoch": 0.4559915164369035,
      "grad_norm": 3.2130398750305176,
      "learning_rate": 2.8751399776035836e-05,
      "loss": 1.5532,
      "step": 860
    },
    {
      "epoch": 0.46129374337221635,
      "grad_norm": 2.8651740550994873,
      "learning_rate": 2.847144456886898e-05,
      "loss": 1.5435,
      "step": 870
    },
    {
      "epoch": 0.46659597030752914,
      "grad_norm": 2.2468349933624268,
      "learning_rate": 2.819148936170213e-05,
      "loss": 1.5443,
      "step": 880
    },
    {
      "epoch": 0.471898197242842,
      "grad_norm": 3.064941167831421,
      "learning_rate": 2.7911534154535274e-05,
      "loss": 1.5342,
      "step": 890
    },
    {
      "epoch": 0.47720042417815484,
      "grad_norm": 1.9333274364471436,
      "learning_rate": 2.7631578947368426e-05,
      "loss": 1.5074,
      "step": 900
    },
    {
      "epoch": 0.48250265111346763,
      "grad_norm": 2.513087511062622,
      "learning_rate": 2.735162374020157e-05,
      "loss": 1.517,
      "step": 910
    },
    {
      "epoch": 0.4878048780487805,
      "grad_norm": 2.841343879699707,
      "learning_rate": 2.7071668533034716e-05,
      "loss": 1.4835,
      "step": 920
    },
    {
      "epoch": 0.4931071049840933,
      "grad_norm": 3.134060859680176,
      "learning_rate": 2.679171332586786e-05,
      "loss": 1.4997,
      "step": 930
    },
    {
      "epoch": 0.4984093319194062,
      "grad_norm": 3.117058753967285,
      "learning_rate": 2.651175811870101e-05,
      "loss": 1.4757,
      "step": 940
    },
    {
      "epoch": 0.503711558854719,
      "grad_norm": 2.969146251678467,
      "learning_rate": 2.6231802911534154e-05,
      "loss": 1.4748,
      "step": 950
    },
    {
      "epoch": 0.5090137857900318,
      "grad_norm": 2.0070297718048096,
      "learning_rate": 2.5951847704367306e-05,
      "loss": 1.4768,
      "step": 960
    },
    {
      "epoch": 0.5143160127253447,
      "grad_norm": 2.319540500640869,
      "learning_rate": 2.567189249720045e-05,
      "loss": 1.4867,
      "step": 970
    },
    {
      "epoch": 0.5196182396606575,
      "grad_norm": 2.5606462955474854,
      "learning_rate": 2.5391937290033596e-05,
      "loss": 1.4553,
      "step": 980
    },
    {
      "epoch": 0.5249204665959704,
      "grad_norm": 2.3790831565856934,
      "learning_rate": 2.511198208286674e-05,
      "loss": 1.4746,
      "step": 990
    },
    {
      "epoch": 0.5302226935312832,
      "grad_norm": 2.7891852855682373,
      "learning_rate": 2.483202687569989e-05,
      "loss": 1.4691,
      "step": 1000
    },
    {
      "epoch": 0.5355249204665959,
      "grad_norm": 2.3724334239959717,
      "learning_rate": 2.4552071668533035e-05,
      "loss": 1.4449,
      "step": 1010
    },
    {
      "epoch": 0.5408271474019088,
      "grad_norm": 3.593883991241455,
      "learning_rate": 2.4272116461366183e-05,
      "loss": 1.449,
      "step": 1020
    },
    {
      "epoch": 0.5461293743372216,
      "grad_norm": 3.7291877269744873,
      "learning_rate": 2.399216125419933e-05,
      "loss": 1.4685,
      "step": 1030
    },
    {
      "epoch": 0.5514316012725344,
      "grad_norm": 2.7444753646850586,
      "learning_rate": 2.3712206047032476e-05,
      "loss": 1.4679,
      "step": 1040
    },
    {
      "epoch": 0.5567338282078473,
      "grad_norm": 3.63988995552063,
      "learning_rate": 2.343225083986562e-05,
      "loss": 1.4461,
      "step": 1050
    },
    {
      "epoch": 0.5620360551431601,
      "grad_norm": 2.5287811756134033,
      "learning_rate": 2.315229563269877e-05,
      "loss": 1.4209,
      "step": 1060
    },
    {
      "epoch": 0.5673382820784729,
      "grad_norm": 3.6568660736083984,
      "learning_rate": 2.2872340425531915e-05,
      "loss": 1.4403,
      "step": 1070
    },
    {
      "epoch": 0.5726405090137858,
      "grad_norm": 2.4617114067077637,
      "learning_rate": 2.2592385218365063e-05,
      "loss": 1.4313,
      "step": 1080
    },
    {
      "epoch": 0.5779427359490986,
      "grad_norm": 3.1299209594726562,
      "learning_rate": 2.231243001119821e-05,
      "loss": 1.4389,
      "step": 1090
    },
    {
      "epoch": 0.5832449628844114,
      "grad_norm": 2.637763261795044,
      "learning_rate": 2.2032474804031356e-05,
      "loss": 1.4243,
      "step": 1100
    },
    {
      "epoch": 0.5885471898197243,
      "grad_norm": 4.436923980712891,
      "learning_rate": 2.1752519596864505e-05,
      "loss": 1.4092,
      "step": 1110
    },
    {
      "epoch": 0.5938494167550371,
      "grad_norm": 2.462954044342041,
      "learning_rate": 2.147256438969765e-05,
      "loss": 1.411,
      "step": 1120
    },
    {
      "epoch": 0.5991516436903499,
      "grad_norm": 4.002403736114502,
      "learning_rate": 2.1192609182530795e-05,
      "loss": 1.4116,
      "step": 1130
    },
    {
      "epoch": 0.6044538706256628,
      "grad_norm": 2.7671799659729004,
      "learning_rate": 2.0912653975363943e-05,
      "loss": 1.3928,
      "step": 1140
    },
    {
      "epoch": 0.6097560975609756,
      "grad_norm": 2.6735455989837646,
      "learning_rate": 2.0632698768197088e-05,
      "loss": 1.3455,
      "step": 1150
    },
    {
      "epoch": 0.6150583244962884,
      "grad_norm": 2.955552816390991,
      "learning_rate": 2.0352743561030236e-05,
      "loss": 1.3644,
      "step": 1160
    },
    {
      "epoch": 0.6203605514316013,
      "grad_norm": 3.385647773742676,
      "learning_rate": 2.0072788353863385e-05,
      "loss": 1.3841,
      "step": 1170
    },
    {
      "epoch": 0.6256627783669141,
      "grad_norm": 2.9607198238372803,
      "learning_rate": 1.979283314669653e-05,
      "loss": 1.3632,
      "step": 1180
    },
    {
      "epoch": 0.630965005302227,
      "grad_norm": 3.7649588584899902,
      "learning_rate": 1.9512877939529675e-05,
      "loss": 1.372,
      "step": 1190
    },
    {
      "epoch": 0.6362672322375398,
      "grad_norm": 3.756338119506836,
      "learning_rate": 1.9232922732362823e-05,
      "loss": 1.3652,
      "step": 1200
    },
    {
      "epoch": 0.6415694591728526,
      "grad_norm": 3.459613561630249,
      "learning_rate": 1.8952967525195968e-05,
      "loss": 1.374,
      "step": 1210
    },
    {
      "epoch": 0.6468716861081655,
      "grad_norm": 3.4483025074005127,
      "learning_rate": 1.8673012318029117e-05,
      "loss": 1.3433,
      "step": 1220
    },
    {
      "epoch": 0.6521739130434783,
      "grad_norm": 3.2919278144836426,
      "learning_rate": 1.8393057110862265e-05,
      "loss": 1.3407,
      "step": 1230
    },
    {
      "epoch": 0.6574761399787911,
      "grad_norm": 4.380887031555176,
      "learning_rate": 1.811310190369541e-05,
      "loss": 1.3559,
      "step": 1240
    },
    {
      "epoch": 0.662778366914104,
      "grad_norm": 3.6060404777526855,
      "learning_rate": 1.7833146696528558e-05,
      "loss": 1.3316,
      "step": 1250
    },
    {
      "epoch": 0.6680805938494168,
      "grad_norm": 3.3022940158843994,
      "learning_rate": 1.7553191489361703e-05,
      "loss": 1.3031,
      "step": 1260
    },
    {
      "epoch": 0.6733828207847296,
      "grad_norm": 4.090967655181885,
      "learning_rate": 1.7273236282194848e-05,
      "loss": 1.34,
      "step": 1270
    },
    {
      "epoch": 0.6786850477200425,
      "grad_norm": 3.223757028579712,
      "learning_rate": 1.6993281075027997e-05,
      "loss": 1.3329,
      "step": 1280
    },
    {
      "epoch": 0.6839872746553552,
      "grad_norm": 3.197924852371216,
      "learning_rate": 1.671332586786114e-05,
      "loss": 1.3205,
      "step": 1290
    },
    {
      "epoch": 0.689289501590668,
      "grad_norm": 3.921247959136963,
      "learning_rate": 1.643337066069429e-05,
      "loss": 1.3245,
      "step": 1300
    },
    {
      "epoch": 0.694591728525981,
      "grad_norm": 4.071775436401367,
      "learning_rate": 1.615341545352744e-05,
      "loss": 1.3227,
      "step": 1310
    },
    {
      "epoch": 0.6998939554612937,
      "grad_norm": 3.516878843307495,
      "learning_rate": 1.5873460246360583e-05,
      "loss": 1.2895,
      "step": 1320
    },
    {
      "epoch": 0.7051961823966065,
      "grad_norm": 3.6602566242218018,
      "learning_rate": 1.559350503919373e-05,
      "loss": 1.3184,
      "step": 1330
    },
    {
      "epoch": 0.7104984093319194,
      "grad_norm": 4.4318037033081055,
      "learning_rate": 1.5313549832026877e-05,
      "loss": 1.309,
      "step": 1340
    },
    {
      "epoch": 0.7158006362672322,
      "grad_norm": 4.379324913024902,
      "learning_rate": 1.5033594624860023e-05,
      "loss": 1.3153,
      "step": 1350
    },
    {
      "epoch": 0.721102863202545,
      "grad_norm": 3.6249144077301025,
      "learning_rate": 1.4753639417693168e-05,
      "loss": 1.2713,
      "step": 1360
    },
    {
      "epoch": 0.7264050901378579,
      "grad_norm": 4.828303813934326,
      "learning_rate": 1.4473684210526317e-05,
      "loss": 1.2989,
      "step": 1370
    },
    {
      "epoch": 0.7317073170731707,
      "grad_norm": 4.387556076049805,
      "learning_rate": 1.4193729003359463e-05,
      "loss": 1.2875,
      "step": 1380
    },
    {
      "epoch": 0.7370095440084835,
      "grad_norm": 4.060532093048096,
      "learning_rate": 1.3913773796192612e-05,
      "loss": 1.2889,
      "step": 1390
    },
    {
      "epoch": 0.7423117709437964,
      "grad_norm": 4.48578405380249,
      "learning_rate": 1.3633818589025757e-05,
      "loss": 1.2849,
      "step": 1400
    },
    {
      "epoch": 0.7476139978791092,
      "grad_norm": 3.8397858142852783,
      "learning_rate": 1.3353863381858902e-05,
      "loss": 1.2596,
      "step": 1410
    },
    {
      "epoch": 0.7529162248144221,
      "grad_norm": 5.397279739379883,
      "learning_rate": 1.3073908174692052e-05,
      "loss": 1.2663,
      "step": 1420
    },
    {
      "epoch": 0.7582184517497349,
      "grad_norm": 3.8277053833007812,
      "learning_rate": 1.2793952967525197e-05,
      "loss": 1.279,
      "step": 1430
    },
    {
      "epoch": 0.7635206786850477,
      "grad_norm": 3.526313543319702,
      "learning_rate": 1.2513997760358342e-05,
      "loss": 1.2499,
      "step": 1440
    },
    {
      "epoch": 0.7688229056203606,
      "grad_norm": 3.6871678829193115,
      "learning_rate": 1.223404255319149e-05,
      "loss": 1.2594,
      "step": 1450
    },
    {
      "epoch": 0.7741251325556734,
      "grad_norm": 4.367236614227295,
      "learning_rate": 1.1954087346024637e-05,
      "loss": 1.242,
      "step": 1460
    },
    {
      "epoch": 0.7794273594909862,
      "grad_norm": 6.099531650543213,
      "learning_rate": 1.1674132138857784e-05,
      "loss": 1.2563,
      "step": 1470
    },
    {
      "epoch": 0.7847295864262991,
      "grad_norm": 3.8294193744659424,
      "learning_rate": 1.139417693169093e-05,
      "loss": 1.2427,
      "step": 1480
    },
    {
      "epoch": 0.7900318133616119,
      "grad_norm": 4.063003063201904,
      "learning_rate": 1.1114221724524077e-05,
      "loss": 1.2347,
      "step": 1490
    },
    {
      "epoch": 0.7953340402969247,
      "grad_norm": 4.821258068084717,
      "learning_rate": 1.0834266517357224e-05,
      "loss": 1.2261,
      "step": 1500
    },
    {
      "epoch": 0.8006362672322376,
      "grad_norm": 4.0391082763671875,
      "learning_rate": 1.055431131019037e-05,
      "loss": 1.2454,
      "step": 1510
    },
    {
      "epoch": 0.8059384941675504,
      "grad_norm": 4.575538158416748,
      "learning_rate": 1.0274356103023517e-05,
      "loss": 1.2413,
      "step": 1520
    },
    {
      "epoch": 0.8112407211028632,
      "grad_norm": 4.54731559753418,
      "learning_rate": 9.994400895856664e-06,
      "loss": 1.2497,
      "step": 1530
    },
    {
      "epoch": 0.8165429480381761,
      "grad_norm": 4.099130153656006,
      "learning_rate": 9.71444568868981e-06,
      "loss": 1.2192,
      "step": 1540
    },
    {
      "epoch": 0.8218451749734889,
      "grad_norm": 3.8674819469451904,
      "learning_rate": 9.434490481522957e-06,
      "loss": 1.2242,
      "step": 1550
    },
    {
      "epoch": 0.8271474019088016,
      "grad_norm": 5.132959365844727,
      "learning_rate": 9.154535274356104e-06,
      "loss": 1.1949,
      "step": 1560
    },
    {
      "epoch": 0.8324496288441146,
      "grad_norm": 5.1331071853637695,
      "learning_rate": 8.87458006718925e-06,
      "loss": 1.2075,
      "step": 1570
    },
    {
      "epoch": 0.8377518557794273,
      "grad_norm": 3.971747636795044,
      "learning_rate": 8.594624860022397e-06,
      "loss": 1.2226,
      "step": 1580
    },
    {
      "epoch": 0.8430540827147401,
      "grad_norm": 3.6695716381073,
      "learning_rate": 8.314669652855544e-06,
      "loss": 1.2108,
      "step": 1590
    },
    {
      "epoch": 0.848356309650053,
      "grad_norm": 5.048966407775879,
      "learning_rate": 8.03471444568869e-06,
      "loss": 1.2235,
      "step": 1600
    },
    {
      "epoch": 0.8536585365853658,
      "grad_norm": 4.016637802124023,
      "learning_rate": 7.754759238521837e-06,
      "loss": 1.2166,
      "step": 1610
    },
    {
      "epoch": 0.8589607635206787,
      "grad_norm": 4.403556823730469,
      "learning_rate": 7.474804031354983e-06,
      "loss": 1.2138,
      "step": 1620
    },
    {
      "epoch": 0.8642629904559915,
      "grad_norm": 4.721828460693359,
      "learning_rate": 7.1948488241881305e-06,
      "loss": 1.2093,
      "step": 1630
    },
    {
      "epoch": 0.8695652173913043,
      "grad_norm": 4.022858619689941,
      "learning_rate": 6.914893617021277e-06,
      "loss": 1.2023,
      "step": 1640
    },
    {
      "epoch": 0.8748674443266172,
      "grad_norm": 4.966745376586914,
      "learning_rate": 6.634938409854425e-06,
      "loss": 1.2047,
      "step": 1650
    },
    {
      "epoch": 0.88016967126193,
      "grad_norm": 3.582756757736206,
      "learning_rate": 6.35498320268757e-06,
      "loss": 1.1982,
      "step": 1660
    },
    {
      "epoch": 0.8854718981972428,
      "grad_norm": 4.11604118347168,
      "learning_rate": 6.075027995520717e-06,
      "loss": 1.1824,
      "step": 1670
    },
    {
      "epoch": 0.8907741251325557,
      "grad_norm": 4.106853485107422,
      "learning_rate": 5.795072788353863e-06,
      "loss": 1.2085,
      "step": 1680
    },
    {
      "epoch": 0.8960763520678685,
      "grad_norm": 4.474435806274414,
      "learning_rate": 5.5151175811870105e-06,
      "loss": 1.1942,
      "step": 1690
    },
    {
      "epoch": 0.9013785790031813,
      "grad_norm": 4.118457317352295,
      "learning_rate": 5.235162374020157e-06,
      "loss": 1.1844,
      "step": 1700
    },
    {
      "epoch": 0.9066808059384942,
      "grad_norm": 4.621235370635986,
      "learning_rate": 4.955207166853304e-06,
      "loss": 1.1976,
      "step": 1710
    },
    {
      "epoch": 0.911983032873807,
      "grad_norm": 3.41998553276062,
      "learning_rate": 4.675251959686451e-06,
      "loss": 1.1959,
      "step": 1720
    },
    {
      "epoch": 0.9172852598091198,
      "grad_norm": 4.069279193878174,
      "learning_rate": 4.3952967525195964e-06,
      "loss": 1.1837,
      "step": 1730
    },
    {
      "epoch": 0.9225874867444327,
      "grad_norm": 4.627798557281494,
      "learning_rate": 4.115341545352744e-06,
      "loss": 1.1815,
      "step": 1740
    },
    {
      "epoch": 0.9278897136797455,
      "grad_norm": 4.515130996704102,
      "learning_rate": 3.835386338185891e-06,
      "loss": 1.1647,
      "step": 1750
    },
    {
      "epoch": 0.9331919406150583,
      "grad_norm": 4.5233893394470215,
      "learning_rate": 3.5554311310190373e-06,
      "loss": 1.1794,
      "step": 1760
    },
    {
      "epoch": 0.9384941675503712,
      "grad_norm": 5.045785427093506,
      "learning_rate": 3.2754759238521836e-06,
      "loss": 1.1906,
      "step": 1770
    },
    {
      "epoch": 0.943796394485684,
      "grad_norm": 4.580410957336426,
      "learning_rate": 2.9955207166853303e-06,
      "loss": 1.185,
      "step": 1780
    },
    {
      "epoch": 0.9490986214209968,
      "grad_norm": 4.109610557556152,
      "learning_rate": 2.7155655095184774e-06,
      "loss": 1.1891,
      "step": 1790
    },
    {
      "epoch": 0.9544008483563097,
      "grad_norm": 4.857969760894775,
      "learning_rate": 2.435610302351624e-06,
      "loss": 1.1822,
      "step": 1800
    },
    {
      "epoch": 0.9597030752916225,
      "grad_norm": 3.6347153186798096,
      "learning_rate": 2.1556550951847707e-06,
      "loss": 1.1717,
      "step": 1810
    },
    {
      "epoch": 0.9650053022269353,
      "grad_norm": 4.294408798217773,
      "learning_rate": 1.8756998880179174e-06,
      "loss": 1.1791,
      "step": 1820
    },
    {
      "epoch": 0.9703075291622482,
      "grad_norm": 5.15295934677124,
      "learning_rate": 1.5957446808510639e-06,
      "loss": 1.1689,
      "step": 1830
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 4.510479927062988,
      "learning_rate": 1.3157894736842106e-06,
      "loss": 1.1738,
      "step": 1840
    },
    {
      "epoch": 0.9809119830328739,
      "grad_norm": 4.292032718658447,
      "learning_rate": 1.0358342665173572e-06,
      "loss": 1.186,
      "step": 1850
    },
    {
      "epoch": 0.9862142099681867,
      "grad_norm": 5.280213832855225,
      "learning_rate": 7.55879059350504e-07,
      "loss": 1.1755,
      "step": 1860
    },
    {
      "epoch": 0.9915164369034994,
      "grad_norm": 6.406702995300293,
      "learning_rate": 4.7592385218365065e-07,
      "loss": 1.1826,
      "step": 1870
    },
    {
      "epoch": 0.9968186638388123,
      "grad_norm": 4.65498161315918,
      "learning_rate": 1.9596864501679733e-07,
      "loss": 1.1786,
      "step": 1880
    }
  ],
  "logging_steps": 10,
  "max_steps": 1886,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 9152893550592.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
