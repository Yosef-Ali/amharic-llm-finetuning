{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amharic LLM Training on Kaggle\n",
    "\n",
    "This notebook trains an enhanced Amharic language model using Kaggle's free GPU resources.\n",
    "\n",
    "## Features:\n",
    "- Amharic-specific tokenization\n",
    "- Mixed precision training\n",
    "- Gradient accumulation\n",
    "- Checkpoint saving to Kaggle datasets\n",
    "- Performance monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets tokenizers accelerate wandb -q\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Model settings\n",
    "    'base_model': 'microsoft/DialoGPT-small',  # Starting point\n",
    "    'max_length': 512,\n",
    "    'vocab_size': 50000,\n",
    "    \n",
    "    # Training settings\n",
    "    'batch_size': 4,\n",
    "    'gradient_accumulation_steps': 8,\n",
    "    'learning_rate': 5e-5,\n",
    "    'num_epochs': 3,\n",
    "    'warmup_steps': 500,\n",
    "    'save_steps': 1000,\n",
    "    'eval_steps': 500,\n",
    "    \n",
    "    # Data settings\n",
    "    'min_text_length': 50,\n",
    "    'max_text_length': 1024,\n",
    "    \n",
    "    # Output settings\n",
    "    'output_dir': '/kaggle/working/amharic_model',\n",
    "    'logging_dir': '/kaggle/working/logs'\n",
    "}\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmharicDataProcessor:\n",
    "    \"\"\"Process Amharic text data for training\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.tokenizer = None\n",
    "    \n",
    "    def load_data(self, data_path=None):\n",
    "        \"\"\"Load Amharic text data\"\"\"\n",
    "        if data_path:\n",
    "            # Load from uploaded dataset\n",
    "            with open(data_path, 'r', encoding='utf-8') as f:\n",
    "                if data_path.endswith('.json'):\n",
    "                    data = json.load(f)\n",
    "                    texts = data.get('texts', [])\n",
    "                else:\n",
    "                    texts = f.read().split('\\n\\n')\n",
    "        else:\n",
    "            # Sample Amharic texts for demonstration\n",
    "            texts = [\n",
    "                \"ሰላም ነው። እንዴት ነዎት? ዛሬ ቆንጆ ቀን ነው።\",\n",
    "                \"ኢትዮጵያ በአፍሪካ ቀንድ የምትገኝ ሀገር ናት። ታሪካዊ እና ባህላዊ ሀብት የበዛባት ሀገር ናት።\",\n",
    "                \"አዲስ አበባ የኢትዮጵያ ዋና ከተማ ናት። በዚህ ከተማ ውስጥ ብዙ ሰዎች ይኖራሉ።\",\n",
    "                \"ትምህርት በሰው ልጅ ህይወት ውስጥ በጣም አስፈላጊ ነው። ትምህርት ሰውን ያበቃዋል።\",\n",
    "                \"ቴክኖሎጂ በዘመናችን ህይወት ውስጥ ትልቅ ሚና ይጫወታል። ኮምፒውተር እና ስማርት ፎን ህይወታችንን ቀይረውታል።\"\n",
    "            ]\n",
    "        \n",
    "        # Filter and clean texts\n",
    "        cleaned_texts = []\n",
    "        for text in texts:\n",
    "            if text and len(text.strip()) >= self.config['min_text_length']:\n",
    "                cleaned_text = text.strip()[:self.config['max_text_length']]\n",
    "                cleaned_texts.append(cleaned_text)\n",
    "        \n",
    "        print(f\"Loaded {len(cleaned_texts)} texts\")\n",
    "        return cleaned_texts\n",
    "    \n",
    "    def setup_tokenizer(self, texts):\n",
    "        \"\"\"Setup Amharic-aware tokenizer\"\"\"\n",
    "        # Load base tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.config['base_model'])\n",
    "        \n",
    "        # Add Amharic-specific tokens if needed\n",
    "        amharic_tokens = ['<amh>', '</amh>', '<question>', '<answer>']\n",
    "        self.tokenizer.add_tokens(amharic_tokens)\n",
    "        \n",
    "        # Set padding token\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(f\"Tokenizer vocabulary size: {len(self.tokenizer)}\")\n",
    "        return self.tokenizer\n",
    "    \n",
    "    def create_dataset(self, texts):\n",
    "        \"\"\"Create training dataset\"\"\"\n",
    "        def tokenize_function(examples):\n",
    "            return self.tokenizer(\n",
    "                examples['text'],\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.config['max_length'],\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = Dataset.from_dict({'text': texts})\n",
    "        tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "        \n",
    "        # Split into train/validation\n",
    "        train_size = int(0.9 * len(tokenized_dataset))\n",
    "        train_dataset = tokenized_dataset.select(range(train_size))\n",
    "        eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "        \n",
    "        print(f\"Training samples: {len(train_dataset)}\")\n",
    "        print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "        \n",
    "        return train_dataset, eval_dataset\n",
    "\n",
    "# Initialize data processor\n",
    "processor = AmharicDataProcessor(CONFIG)\n",
    "print(\"Data processor initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KaggleAmharicTrainer:\n",
    "    \"\"\"Kaggle-optimized Amharic model trainer\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self.trainer = None\n",
    "    \n",
    "    def setup_model(self, tokenizer):\n",
    "        \"\"\"Setup model for Amharic training\"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # Load base model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.config['base_model'],\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        \n",
    "        # Resize token embeddings for new tokens\n",
    "        self.model.resize_token_embeddings(len(tokenizer))\n",
    "        \n",
    "        print(f\"Model loaded: {self.config['base_model']}\")\n",
    "        print(f\"Model parameters: {self.model.num_parameters():,}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def setup_training(self, train_dataset, eval_dataset):\n",
    "        \"\"\"Setup training configuration\"\"\"\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.config['output_dir'],\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=self.config['num_epochs'],\n",
    "            per_device_train_batch_size=self.config['batch_size'],\n",
    "            per_device_eval_batch_size=self.config['batch_size'],\n",
    "            gradient_accumulation_steps=self.config['gradient_accumulation_steps'],\n",
    "            learning_rate=self.config['learning_rate'],\n",
    "            warmup_steps=self.config['warmup_steps'],\n",
    "            logging_steps=100,\n",
    "            save_steps=self.config['save_steps'],\n",
    "            eval_steps=self.config['eval_steps'],\n",
    "            evaluation_strategy='steps',\n",
    "            save_strategy='steps',\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model='eval_loss',\n",
    "            greater_is_better=False,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            dataloader_pin_memory=False,\n",
    "            remove_unused_columns=False,\n",
    "            logging_dir=self.config['logging_dir']\n",
    "        )\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False  # Causal LM\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer\n",
    "        )\n",
    "        \n",
    "        print(\"Training setup completed\")\n",
    "        return self.trainer\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Start training\"\"\"\n",
    "        print(\"Starting training...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Train the model\n",
    "        train_result = self.trainer.train()\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        training_time = end_time - start_time\n",
    "        \n",
    "        print(f\"Training completed in: {training_time}\")\n",
    "        print(f\"Final training loss: {train_result.training_loss:.4f}\")\n",
    "        \n",
    "        return train_result\n",
    "    \n",
    "    def save_model(self, save_path=None):\n",
    "        \"\"\"Save trained model\"\"\"\n",
    "        if save_path is None:\n",
    "            save_path = self.config['output_dir']\n",
    "        \n",
    "        # Save model and tokenizer\n",
    "        self.trainer.save_model(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        \n",
    "        # Save training config\n",
    "        config_path = Path(save_path) / 'training_config.json'\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(self.config, f, indent=2)\n",
    "        \n",
    "        print(f\"Model saved to: {save_path}\")\n",
    "        return save_path\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = KaggleAmharicTrainer(CONFIG)\n",
    "print(\"Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load and process data\n",
    "print(\"=== Step 1: Data Loading ===")\n",
    "texts = processor.load_data()  # Add your data path here\n",
    "\n",
    "# Step 2: Setup tokenizer\n",
    "print(\"\\n=== Step 2: Tokenizer Setup ===")\n",
    "tokenizer = processor.setup_tokenizer(texts)\n",
    "\n",
    "# Step 3: Create datasets\n",
    "print(\"\\n=== Step 3: Dataset Creation ===")\n",
    "train_dataset, eval_dataset = processor.create_dataset(texts)\n",
    "\n",
    "# Step 4: Setup model\n",
    "print(\"\\n=== Step 4: Model Setup ===")\n",
    "model = trainer.setup_model(tokenizer)\n",
    "\n",
    "# Step 5: Setup training\n",
    "print(\"\\n=== Step 5: Training Setup ===")\n",
    "trainer_obj = trainer.setup_training(train_dataset, eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Start training\n",
    "print(\"=== Step 6: Training ===")\n",
    "train_result = trainer.train()\n",
    "\n",
    "# Step 7: Save model\n",
    "print(\"\\n=== Step 7: Model Saving ===")\n",
    "model_path = trainer.save_model()\n",
    "\n",
    "print(\"\\n🎉 Training completed successfully!\")\n",
    "print(f\"📁 Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "def test_model(model, tokenizer, prompt, max_length=100):\n",
    "    \"\"\"Test the trained model with a prompt\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Encode prompt\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.8,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"ሰላም\",\n",
    "    \"ኢትዮጵያ\",\n",
    "    \"ትምህርት\",\n",
    "    \"ቴክኖሎጂ\"\n",
    "]\n",
    "\n",
    "print(\"=== Model Testing ===")\n",
    "for prompt in test_prompts:\n",
    "    result = test_model(model, tokenizer, prompt)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated: {result}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create deployment package\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def create_deployment_package():\n",
    "    \"\"\"Create a deployment package\"\"\"\n",
    "    deployment_dir = '/kaggle/working/deployment'\n",
    "    os.makedirs(deployment_dir, exist_ok=True)\n",
    "    \n",
    "    # Copy model files\n",
    "    model_files = ['pytorch_model.bin', 'config.json', 'tokenizer.json', \n",
    "                   'tokenizer_config.json', 'vocab.txt', 'training_config.json']\n",
    "    \n",
    "    for file in model_files:\n",
    "        src = Path(CONFIG['output_dir']) / file\n",
    "        if src.exists():\n",
    "            shutil.copy2(src, deployment_dir)\n",
    "    \n",
    "    # Create deployment script\n",
    "    deploy_script = '''\\\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "class AmharicGenerator:\n",
    "    def __init__(self, model_path):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "        self.model.eval()\n",
    "    \n",
    "    def generate(self, prompt, max_length=100):\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\\"pt\\")\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs, max_length=max_length, \n",
    "                temperature=0.8, do_sample=True\n",
    "            )\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Usage:\n",
    "# generator = AmharicGenerator(\\".\\")\n",
    "# result = generator.generate(\\"ሰላም\\")\n",
    "'''\n",
    "    \n",
    "    with open(f'{deployment_dir}/amharic_generator.py', 'w') as f:\n",
    "        f.write(deploy_script)\n",
    "    \n",
    "    # Create ZIP package\n",
    "    zip_path = '/kaggle/working/amharic_model_deployment.zip'\n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        for root, dirs, files in os.walk(deployment_dir):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                arcname = os.path.relpath(file_path, deployment_dir)\n",
    "                zipf.write(file_path, arcname)\n",
    "    \n",
    "    print(f\\"Deployment package created: {zip_path}\\")\n",
    "    return zip_path\n",
    "\n",
    "# Create deployment package\n",
    "deployment_package = create_deployment_package()\n",
    "\n",
    "print(\\"\\\\n✅ Training and packaging completed!\\")\n",
    "print(f\\"📦 Deployment package: {deployment_package}\\")\n",
    "print(\\"\\\\n📋 Next steps:\\")\n",
    "print(\\"1. Download the deployment package\\")\n",
    "print(\\"2. Upload to Hugging Face Spaces\\")\n",
    "print(\\"3. Deploy to GitHub Pages with ONNX.js\\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}